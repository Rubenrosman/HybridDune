{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5ebbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETCDF SAVING AND COMPRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279f6930",
   "metadata": {},
   "source": [
    "# NetCDF saving and compression\n",
    "\n",
    "There are several options to decrease the file size of NetCDFs:\n",
    "1) Enabling compression\n",
    "<Br>Basically zipping variables\n",
    "2) Enabling compression with shuffle setting. \n",
    "<Br> A setting for compression that usually decreases data size\n",
    "3) Setting data type\n",
    "<Br> Saving data with e.g. single precision instead of double precision, or int8 instead of uint64\n",
    "4) Setting precision\n",
    "<Br> Saving data with e.g. 2 decimals precision, instead of the full floating point precision\n",
    "5) Chunksize\n",
    "<Br> Setting the order in which muldi-dimensional arrays are saved, to reduce variation between consequetive observations and increase compression efficiency\n",
    "- ...\n",
    "\n",
    "Note: option 1, 2 and 5 only affect the netcdf size (and the reading and writing speed). Option 2 and 3 also change the data (lossless vs lossy compression)\n",
    "These options will be demonstrated below. To do so an ADV output file is used, using the example data from the TUD-COASTAL GITHUB.\n",
    "\n",
    "This file can be downloaded from a public Google Drive: \n",
    "https://drive.google.com/drive/folders/1-o7MemJYKXbmKTThVin0ze6XrBeXhEP9?usp=sharing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dd4f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45305402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "ncDir = r\"O:\\HybridDune experiment\\data ADV, OBS\\TUD-Coastal\"\n",
    "\n",
    "file_in = r\"vec1_pilot.nc\"\n",
    "ds = xr.open_dataset(os.path.join(ncDir, file_in))\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd52b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: export uncompressed\n",
    "nc_out = os.path.join(ncDir, \"vec1_pilot_out0_uncompressed.nc\") \n",
    "ds.to_netcdf(nc_out)\n",
    "# Filesize: 531 MB\n",
    "\n",
    "# drop some of the variables, to make selecting settings per variable easier\n",
    "ds = ds.drop_vars(['v', 'w', 'anl2', 'a2', 'a3', 'cor2', 'cor3', 'snr2', 'snr3'])\n",
    "nc_out = os.path.join(ncDir, \"vec1_pilot_out0b_uncompressed, selection.nc\") \n",
    "ds.to_netcdf(nc_out)   # 292 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb82877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Enabling compression\n",
    "comp = dict(zlib=True, complevel=4) # apply deflate level 4 compression\n",
    "encoding = {var: comp for var in list(ds.data_vars) + list(ds.coords)}  # dictionary with compression settings\n",
    "ds.encoding=encoding # save applied encoding to netcdf. Not necessary, but useful to keep track of applied encoding and retrieve it lateron\n",
    "\n",
    "nc_out = os.path.join(ncDir, \"vec1_pilot_out1_compressed.nc\") \n",
    "ds.to_netcdf(nc_out, encoding=ds.encoding) # NB: need to pass encoding when calling to_netcdf, ds.encoding is not applied automatically\n",
    "# filesize 41 MB\n",
    "\n",
    "# NB: compression should be applied to the variables and coordinates. If you forget the coordinates, part of the file remains uncompressed. (This is especially important for files \n",
    "# with as many coordinates as samples (e.g. 1,000,000 datetimes with pressure), in this case with blocked data (t x N, each relatively short) it is less important)\n",
    "\n",
    "# Note: Deflate compression level 4 is used here. You can increase the number for stronger compression, lower the number for faster (de)compression. Range 0-9. (de)compression times \n",
    "# rise munch faster than the file size reduces...\n",
    "# XArray also supports other compression algorithms. This is not advised for general use, as it reduces the compatibility with other software. Matlab for instance supports \n",
    "# (to my knowledge) only deflate compression. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f6bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Enabling compression, setting shuffle\n",
    "# Shuffle determines how the data is rearranged before compression.  \n",
    "# Technically, it determines the bit order: whether the full value of each data point is stored before the \n",
    "# next point is stored, or if instead the first bit of every data point is stored, then the second bit, etc. \n",
    "# More practically, often it improves compression efficiency, sometimes is makes it worse. So it can be worth testing. Or even setting it per variable, instead of the same for all.\n",
    "# Note: by default shuffle is on when deflate compression is applied in xarray exports (when zlib=True, complever=#). For instance Matlabs ncwrite has it off by default. \n",
    "\n",
    "# Try shuffle on -----------------------------------------\n",
    "comp = dict(zlib=True, complevel=4, shuffle=True)                       # apply deflate level 4 compression and enable shuffle\n",
    "encoding = {var: comp for var in list(ds.data_vars) + list(ds.coords)}  # dictionary with compression settings\n",
    "ds.encoding=encoding                                                    # save applied encoding to netcdf. Useful for retrieval\n",
    "\n",
    "nc_out = os.path.join(ncDir, \"vec1_pilot_out2 compressed_shuffle.nc\") \n",
    "ds.to_netcdf(nc_out, encoding=ds.encoding)                              # NB: need to pass encoding when calling to_netcdf, ds.encoding is not applied automatically\n",
    "# filesize = 41 MB\n",
    "\n",
    "# Try shuffle off -----------------------------------------\n",
    "comp = dict(zlib=True, complevel=4, shuffle=False)                      # apply deflate level 4 compression and disable shuffle\n",
    "encoding = {var: comp for var in list(ds.data_vars) + list(ds.coords)}  # dictionary with compression settings\n",
    "ds.encoding=encoding                                                    # save applied encoding to netcdf. Useful for retrieval\n",
    "\n",
    "nc_out = os.path.join(ncDir, \"vec1_pilot_out2 compressed_no_shuffle.nc\") \n",
    "ds.to_netcdf(nc_out, encoding=ds.encoding) # NB: need to pass encoding when calling to_netcdf, ds.encoding is not applied automatically\n",
    "# filesize = 26 MB\n",
    "\n",
    "# Set shuffle per variable -----------------------------------------\n",
    "# NB: Only set for the variables with large data size (t x N)\n",
    "\n",
    "# First define a custom encoding dictionary for the (large) dataset variables, to select the shuffle setting per variable\n",
    "encoding = {'p': {'shuffle': True},    # shuffle flag based on trial and error per variable, to determine which settings works best\n",
    "            'u': {'shuffle': False},\n",
    "            'anl1': {'shuffle': True},\n",
    "            'a1': {'shuffle': True},\n",
    "            'cor1': {'shuffle': True},\n",
    "            'snr1': {'shuffle': False},\n",
    "            'voltage': {'shuffle': False},\n",
    "            'heading': {'shuffle': True},\n",
    "            'burst': {'shuffle': True} }\n",
    "\n",
    "# Then extend the dictionary: add deflate compression level 4 to all variables and coordinates in netCDF, without overwriting existing keys\n",
    "compression = {var: {\"zlib\": True, \"complevel\": 4} for var in list(ds.data_vars) + list(ds.coords)}  # temporary dict, with only compression settings\n",
    "for var, comp in compression.items():  # for each variable in the dataset, \n",
    "    if var in encoding:                # if the variable already has an encoding, update it with the compression settings\n",
    "        encoding[var].update(comp)\n",
    "    else:                              # if the variable does not have an encoding yet, add it \n",
    "        encoding[var] = comp\n",
    "ds.encoding=encoding                   # save applied encoding to netcdf. Useful for retrieval\n",
    "\n",
    "nc_out = os.path.join(ncDir, \"vec1_pilot_out2 compressed_best_shuffle.nc\") \n",
    "ds.to_netcdf(nc_out, encoding=ds.encoding) # NB: need to pass encoding when calling to_netcdf, ds.encoding is not applied automatically\n",
    "# filesize = 23 MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037de880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dpoppema\\AppData\\Local\\anaconda3\\envs\\Lidar\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3699: SerializationWarning: saving variable anl1 with floating point data as an integer dtype without any _FillValue to use for NaNs\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "c:\\Users\\dpoppema\\AppData\\Local\\anaconda3\\envs\\Lidar\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3699: SerializationWarning: saving variable cor1 with floating point data as an integer dtype without any _FillValue to use for NaNs\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# 3) Setting data type\n",
    "# NB: setting the data type CHANGES the data, it REMOVES information/precision. \n",
    "# Changing the data from double to single precision (float64 to float32) removes some precision and removes the ability to represent very large values. Make sure \n",
    "# you understand what you do! For integer data types, this is even more important: by definition, they cannot store floating point numbers, nor nan or inf. And they \n",
    "# have a very defined range (e.g. int8 can only store values from -128 to 127).\n",
    "\n",
    "# Define a dictionary where the data type is set to float32, or appropiate integer type. \n",
    "encoding = {'p': {'dtype': 'float32'}, \n",
    "            'u': {'dtype': 'float32'},\n",
    "            'anl1': {'dtype': 'uint16'}, # data consists of integers, 0-65535, so uint16\n",
    "            'a1': {'dtype': 'float32'},\n",
    "            'cor1': {'dtype': 'int8'},   # correlaton, 0-100% as integers, so int8\n",
    "            'snr1': {'dtype': 'float32'},\n",
    "            'voltage': {'dtype': 'float32'},\n",
    "            'heading': {'dtype': 'float32'},\n",
    "            'burst': {'dtype': 'float32'} }\n",
    "\n",
    "# Then extend the dictionary: add deflate compression level 4 to all variables and coordinates in netCDF, without overwriting existing keys. Use no shuffle\n",
    "compression = {var: {\"zlib\": True, \"complevel\": 4, \"shuffle\": False} for var in list(ds.data_vars) + list(ds.coords)}  # temporary dict, with only compression settings\n",
    "for var, comp in compression.items():  # for each variable in the dataset, \n",
    "    if var in encoding:                # if the variable already has an encoding, update it with the compression settings\n",
    "        encoding[var].update(comp)\n",
    "    else:                              # if the variable does not have an encoding yet, add it \n",
    "        encoding[var] = comp\n",
    "ds.encoding=encoding                   # save applied encoding to netcdf. Useful for retrieval\n",
    "\n",
    "nc_out = os.path.join(ncDir, \"vec1_pilot_out3 compressed_single_precision_no_shuffle.nc\") \n",
    "ds.to_netcdf(nc_out, encoding=ds.encoding) # NB: need to pass encoding when calling to_netcdf, ds.encoding is not applied automatically\n",
    "# Size 20 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9983ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dpoppema\\AppData\\Local\\anaconda3\\envs\\Lidar\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3699: SerializationWarning: saving variable anl1 with floating point data as an integer dtype without any _FillValue to use for NaNs\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# 4) Setting precision of variables. By using the precision you need and not more, you can save storage space. But make sure you know understand you do! You need to know both \n",
    "# the precision you need, AND the range of values in the data.\n",
    "\n",
    "# Precision works by multiplying the data with a factor (e.g. 100 for 2 decimals), rounding it to integer, and saving it as an integer data type. Make sure your integer data type can\n",
    "# handle the range of values in your data. For instance, uint8 can only store values from 0 to 255. So if you are saving 2 decimals, it can handle values between 0 and 2.55 only.\n",
    "\n",
    "# Note: this setting can be especially useful to match the precision of original data in raw text files. With e.g. 2 decimals in the original textfile, you can use 2 decimals for the \n",
    "# netcdf to effectively compress losslessly. # BUT THIS ONLY WORKS IF you make sure you handle the range, missing data etc correctly.  \n",
    "\n",
    "# Define a custom dictionary to set the scale factor and data type per variable. \n",
    "# Note: I have followed the original data precision to avoid loss of information. Alternatively, you could reduce the precision for some vars, according to your needs\n",
    "# Note 2: You don't need to do this for all the variables. You can also do it for specific ones that are very large, or suitable for lower precision. Here I defined precision \n",
    "# for all vars with t,N data (they are the largest), skipped the others. \n",
    "encoding = {'p': { 'scale_factor': 10.0, 'dtype': 'uint16', '_FillValue': 0, 'shuffle': False},      # Original data multiples of 10, so scale factor 10.0. NB: scale factor 10.0, not 10, to make unpacked p float (able to contain NaN)\n",
    "            'u': { 'scale_factor': 0.001, 'dtype': 'int16', '_FillValue': -9999, 'shuffle': False},  # three decimals originally, so scale factor 0.001. max value is 7m/s, with 3 decimals is 7000 options, so int16 scale of ± 32767 is sufficient\n",
    "            'anl1': { 'dtype': 'uint16'},     # default flag is shuffle true, can be skipped         # analog connection, OBS. Orignal data in counts (int). Possible range 0-65535, so exactly uint16\n",
    "            'a1': { 'dtype': 'int16', '_FillValue': -9999},                                          # Amplitude beam 1. Original data in ints.  Seemingly 0-~160. Use int16 to be sure\n",
    "            'cor1': { 'dtype': 'int8', '_FillValue': -99},                                           # Correlation beam 1. Original data in int, range 0-100#. So int8 is sufficient \n",
    "            'snr1': { 'scale_factor': 0.1, 'dtype': 'int16', '_FillValue': -9999, 'shuffle': False}, # SNR beam 1. Original 1 decimal, range 0-65. So int16 is sufficient\n",
    "            'voltage': { 'scale_factor': 0.1, 'dtype': 'int8', '_FillValue': -99, 'shuffle': False}, # Original 1 decimal, with in this case 0-12V. So int8 is sufficient.   \n",
    "            'heading': { 'scale_factor': 0.1, 'dtype': 'int16', '_FillValue': -9999},                # Original 1 decimal, range 0-360. So int16 is sufficient\n",
    "            'burst': { 'scale_factor': 1.0, 'dtype': 'int16', '_FillValue': -9999} }                 # burst: original int. int16 chosen, to be able to handle long deployments/short bursts\n",
    "\n",
    "# Then extend the dictionary: add deflate compression level 4 to all variables and coordinates in netCDF, without overwriting existing keys. \n",
    "compression = {var: {\"zlib\": True, \"complevel\": 4} for var in list(ds.data_vars) + list(ds.coords)}  # temporary dict, with only compression settings\n",
    "for var, comp in compression.items():  # for each variable in the dataset, \n",
    "    if var in encoding:                # if the variable already has an encoding, update it with the compression settings\n",
    "        encoding[var].update(comp)\n",
    "    else:                              # if the variable does not have an encoding yet, add it \n",
    "        encoding[var] = comp\n",
    "ds.encoding=encoding                   # save applied encoding to netcdf. Useful for retrieval\n",
    "\n",
    "nc_out = os.path.join(ncDir, \"vec1_pilot_out4 compressed_set_precision_and_data_type.nc\") \n",
    "ds.to_netcdf(nc_out, encoding=ds.encoding) # NB: need to pass encoding when calling to_netcdf, ds.encoding is not applied automatically\n",
    "# 17 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adcd55e",
   "metadata": {},
   "source": [
    "Some remarks on setting precision\n",
    "- Packing the data only works when storing vars as int. If data is stored as floating point, all decimals are still stored, and nothing changes in the precision or file size. \n",
    "- _FillValue: any nan in the data will be stored as _FillValue. Without the _FillValue setting, data cannot contain nans. Make sure your data does not contain data equal to the fillvalue. \n",
    "    - If the voltage would e.g. have scale_factor=0.1 and _FillValue=99, any data point with value 9.9 (so 99 after applying the scale factor) would be stored as nan! Note: for the applied settings for p, with FillValue=0, any pressure that was originally zero is replaced by NaN. \n",
    "    - Without FillValue, NaNs cannot be stored. For anl1 this is the case. THIS IS NOT ADVISED! Python actually gives a warning for this. If possible, use the min or max value of the int class for nans. Or\n",
    "consider using a 'higher' class, int32 in this case. This also makes next processing steps safer: nans may be needed then to filter data (and people may forget to convert data to a type able to contain nans).\n",
    "- If data is not centered around zero (e.g. pressure), you can use add_offset to center it around zero, to be able to use a smaller integer data type. See the links below.\n",
    "- Note that scale_factor and add_offset must be of same type and determine the type of the unpacked data. \n",
    "    - If you set scale_factor=1 (don't store decimals), the scale factor is a int type. So data will be unpacked to int. This means it cannot contain nans, and obs stored as _FillValue are converted to zero. \n",
    "    - If you set the scale factor as float, e.g. 1.0, data is unpacked as floating point (even though it is stored as int). So anything stored as FillValue is unpacked as nan.  \n",
    "\n",
    "For more info, see https://docs.xarray.dev/en/stable/user-guide/io.html, especially the text under \"\"Writing encoded data\", and https://cfconventions.org/Data/cf-conventions/cf-conventions-1.11/cf-conventions.html#packed-data The second link also gives some additional options for compression, such as sparse data. However, they are more difficult to apply and make reading the data harder and software-specific, so I suggest not using them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e892ff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Setting chunk size, dim order\n",
    "# Data is not compressed for an enitre dataset at once. It is compressed per chunk. You can see this as writing tiny zip files for each chunk. \n",
    "# If you have a t * x dataset of 100 timesteps (rows) * 10 locations (columns), you could pack everthing togeter (1 chunk), split the file per row (100 chunks of 10 values), per column \n",
    "# (10 chunks of 100 values), # or any other combination (e.g. chunks of 2x5 obs). And if a chunk contains multiple dimensions, it matters in which order the dimensions are stored in the \n",
    "# chunk.\n",
    "\n",
    "# The chunk size not only affects the compression efficiency (file size), but also the reading and writing speed. This is especially important if you only  need a subset of the \n",
    "# data at once. For example, for our 100*10 t * x dataset, you only need data for the first timestep, so 10 obs. If this was chunked per timestep, you only need to load one chunk \n",
    "# with 10 obs. # If it was chunked per location, you need to load 10 chunks of 100 timesteps each, so 1000 obs instead of 10. This is especially important for very large datasets, \n",
    "# that do not fit into memory, and are by definition read in parts.\n",
    "\n",
    "# For demonstration, we use several chunk settings. We will combine them with deflate compression, no shuffle, no special data types or precision settings. \n",
    "\n",
    "# First define a custom encoding dictionary for the (large) dataset variables, to set the chunk size per variable\n",
    "for i in [0,1 ]:  # try different chunk sizes and orders\n",
    "    if i == 0:\n",
    "        chunksize_1 = 1     # 1 t per chunk\n",
    "        chunksize_2 = 9600  # 300 N per chunk (all)\n",
    "        filename_out = \"vec1_pilot_out5a compressed_chunk_1_9600.nc\"  # 27 MB\n",
    "    elif i == 1:\n",
    "        chunksize_1 = 1     # 1 t per chunk\n",
    "        chunksize_2 = 300  # 300 N per chunk (all)\n",
    "        filename_out = \"vec1_pilot_out5b compressed_chunk_1_300.nc\"   # 45 MB, so smaller than similar size of t-chunks (5c)\n",
    "    elif i == 2:\n",
    "        chunksize_1 = 354  # t: all\n",
    "        chunksize_2 = 1    # N\n",
    "        filename_out = \"vec1_pilot_out5c compressed_chunk_354_1.nc\"   # 57 MB\n",
    "    elif i == 3:\n",
    "        chunksize_1 = 59   # t  \n",
    "        chunksize_2 = 1600 # N\n",
    "        filename_out = \"vec1_pilot_out5d compressed_chunk_59_1600.nc\" # 27 MB, so dataset with t,N order is smaller than N,t order (5e)\n",
    "    elif i == 4:\n",
    "        # Switch the order of the dimensions t and N around, to show effect on saving\n",
    "        ds=ds.transpose(\"N\", \"t\") # syntax: order dimensions in ds: N is the first dim, t the second\n",
    "        chunksize_1 = 1600 # N\n",
    "        chunksize_2 = 59   # t\n",
    "        filename_out = \"vec1_pilot_out5e compressed_chunk_59_1600_reordered.nc\" # 44 MB\n",
    "    elif i == 5: \n",
    "        ds=ds.transpose(\"t\", \"N\") # original order of dimensions: t, then N\n",
    "        chunksize_1 = 354  # t: all\n",
    "        chunksize_2 = 9600 # N: all\n",
    "        filename_out = \"vec1_pilot_out5f compressed_chunk_354_9600.nc\"  # 26 MB\n",
    "\n",
    "    encoding = {'p': {'chunksizes': (chunksize_1, chunksize_2)}, \n",
    "                'u': {'chunksizes': (chunksize_1, chunksize_2)},\n",
    "                'anl1': {'chunksizes': (chunksize_1, chunksize_2)},\n",
    "                'a1': {'chunksizes': (chunksize_1, chunksize_2)},\n",
    "                'cor1': {'chunksizes': (chunksize_1, chunksize_2)},\n",
    "                'snr1': {'chunksizes': (chunksize_1, chunksize_2)},\n",
    "                'voltage': {'chunksizes': (chunksize_1, chunksize_2)},\n",
    "                'heading': {'chunksizes': (chunksize_1, chunksize_2)},\n",
    "                'burst': {'chunksizes': (chunksize_1, chunksize_2)} }\n",
    "    \n",
    "    # Then extend the dictionary: add deflate compression level 4 to all variables and coordinates in netCDF, without overwriting existing keys\n",
    "    compression = {var: {\"zlib\": True, \"complevel\": 4, \"shuffle\": False} for var in list(ds.data_vars) + list(ds.coords)}  # temporary dict, with only compression settings\n",
    "    for var, comp in compression.items():  # for each variable in the dataset, \n",
    "        if var in encoding:                # if the variable already has an encoding, update it with the compression settings\n",
    "            encoding[var].update(comp)\n",
    "        else:                              # if the variable does not have an encoding yet, add it \n",
    "            encoding[var] = comp\n",
    "    ds.encoding=encoding                   # save applied encoding to netcdf. Useful for retrieval\n",
    "\n",
    "    nc_out = os.path.join(ncDir, filename_out) \n",
    "    ds.to_netcdf(nc_out, encoding=ds.encoding) # NB: need to pass encoding when calling to_netcdf, ds.encoding is not applied automatically\n",
    "\n",
    "# Note: data varies much slower over N (1/16 of a second apart) than over t (10min apart). Therefore:\n",
    "# - For storage efficiency, single-dim chunks should be over the variable with limited variation (N). See 5b vs 5c.   Note: for data usage instead of storage, it depends on your needs...\n",
    "# - for multi-dim chunks, order matters. For XArray, compression is better if the last chunk dimension compresses easily. (5d vs 5e)\n",
    "# Also: larger chunks allow for better compression (5a vs 5b, with 300 vs 9600 N per chunk). Of course, they slow down reading small parts of the data...\n",
    "\n",
    "# Overall, the important question is which dimension(s) a chunk should contain. How large the chunk is exactly is secondary. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3b57d3",
   "metadata": {},
   "source": [
    "# SUMMARY\n",
    "There are many ways to compress NetCDF files. Remember, the main goal is retaining all the necessary data. A small file size is secondary. You don't want to introduce clipping, bad precision, loss of infs/nans, or other problems in your compression. Nor excessively slow exporting/importing (or too much coding effort)\n",
    "\n",
    "At a minimum, apply compression to the variables *and coordinates* in your file. A quick second step is checking if a general shuffle=on or shuffle=off is better. \n",
    "\n",
    "You can play with data types or precision. Make sure you kwow what you do. Concentrate on the variables that require a lot of space, you don't need to do this for all variables. \n",
    "\n",
    "Changing chunking settings is especially useful for large files. As a quick first step, you can check if the last dimension in the dataset is the dimension with slow changes. And don't forget about data reading: you don't want to read a full file to access a small part of it. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lidar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
