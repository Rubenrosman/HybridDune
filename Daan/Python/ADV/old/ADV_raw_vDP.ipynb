{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(r'C:\\Users\\dpoppema\\Documents\\GitHub\\HybridDune\\Ruben\\ADV')\n",
    "from vector import Vector\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of raw data\n",
    "dataFolder = r'O:\\HybridDune experiment\\data ADV, OBS\\ADV RWS1\\Deployment 1, until 23dec'\n",
    "dataFolder = r'O:\\HybridDune experiment\\data ADV, OBS\\ADV RWS2\\Deployment 1, until 23dec'\n",
    "dataFolder = r'O:\\HybridDune experiment\\data ADV, OBS\\ADV RWS3\\S3, Deployment1, until dec23'\n",
    "dataFolder = r'O:\\HybridDune experiment\\data ADV, OBS\\ADV RWS4\\Deployment until 23dec'\n",
    "dataFolder = r'O:\\HybridDune experiment\\data ADV, OBS\\ADV TUD10\\Deployment until 23dec'\n",
    "\n",
    "# name of the instantiated vector class\n",
    "name = 'ADV_RWS4_Deployment1'\n",
    "name = 'ADV_TUD1_Deployment1'\n",
    "\n",
    "# start time over which to read data (must be larger than first recorded time)\n",
    "tstart = '2024-12-16 13:00:00'\n",
    "tstart = '2024-12-13 10:00:00'\n",
    "\n",
    "# stop time over which to read data (must be smaller than last recorded time)\n",
    "tstop = '2024-12-23 13:00:00'\n",
    "tstop = '2024-12-23 19:00:00'\n",
    "\n",
    "# make map 'raw_netcdf' in dataFolder and set this as output directory\n",
    "filename_out = 'ADV_RWS4_Deployment1.nc'\n",
    "filename_out = 'ADV_TUD1_Deployment1.nc'\n",
    "\n",
    "ncOutDir = r'O:\\HybridDune experiment\\data ADV, OBS\\raw NetCDF'\n",
    "\n",
    "if not os.path.exists(ncOutDir):\n",
    "    os.makedirs(ncOutDir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dpoppema\\Documents\\GitHub\\HybridDune\\Ruben\\ADV\\vector.py:219: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  self.burstStartTimes = pd.date_range(start = self.tstart,periods = nBursts,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".dat file was read\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dpoppema\\Documents\\GitHub\\HybridDune\\Ruben\\ADV\\vector.py:288: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  timeSen = pd.date_range(start = self.tstart, periods = nSamples,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".sen file was read\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dpoppema\\Documents\\GitHub\\HybridDune\\Ruben\\ADV\\vector.py:323: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  df2 = df2.resample('{}S'.format(1/self.frequency)).asfreq()\n",
      "C:\\Users\\dpoppema\\Documents\\GitHub\\HybridDune\\Ruben\\ADV\\vector.py:330: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df3 = df3.fillna(method='ffill')\n",
      "C:\\Users\\dpoppema\\AppData\\Local\\Temp\\ipykernel_16736\\2669706652.py:67: SerializationWarning: saving variable anl1 with floating point data as an integer dtype without any _FillValue to use for NaNs\n",
      "  ds.to_netcdf(os.path.join(ncOutDir, filename_out), encoding=encoding)\n",
      "C:\\Users\\dpoppema\\AppData\\Local\\Temp\\ipykernel_16736\\2669706652.py:67: SerializationWarning: saving variable anl2 with floating point data as an integer dtype without any _FillValue to use for NaNs\n",
      "  ds.to_netcdf(os.path.join(ncOutDir, filename_out), encoding=encoding)\n"
     ]
    }
   ],
   "source": [
    "# raw data to netcdf\n",
    "vec = Vector(name, dataFolder, tstart=tstart, tstop=tstop)\n",
    "\n",
    "# reads the raw data from tstart to tstop and casts all data in a pandas DataFrame that is stored under vec.dfpuv.\n",
    "# in case there is no data between tstart and tstop the DataFrame is not instantiated\n",
    "vec.read_raw_data()\n",
    "\n",
    "# break up the data into burst blocks\n",
    "vec.cast_to_blocks_in_xarray(blockWidth=1200) # 1200 seconds = 20 minutes  # Daan: blocks turn out 1 hour. Maybe because every hour misses 1 minute/10s/something? CHECK!\n",
    "\n",
    "# compute burst averages (make sure to read vector.py what is happening exactly!)\n",
    "vec.compute_block_averages()\n",
    "\n",
    "# all data is collected in an xarray Dataset ds. We extract this from the class instantiation and\n",
    "# we can easily write it to netCDF\n",
    "ds = vec.ds\n",
    "\n",
    "# add global attribute metadata\n",
    "ds.attrs = {'Conventions': 'CF-1.6',\n",
    "            'title': '{}'.format(vec.name),\n",
    "            'instrument': '{}'.format('vec1'),\n",
    "            'instrument serial number': '{}'.format(16725),\n",
    "            'epsg': 28992,\n",
    "             'x': 117196.6,\n",
    "            'y': 559818.2,\n",
    "            'time zone': 'UTC+2',\n",
    "            'coordinate type': 'XYZ',\n",
    "            'summary': 'hybrid-Dune field campaign',\n",
    "            'contact person': 'Ruben Rosman ',\n",
    "            'emailadres': 'r.g.c.rosman@student.tudelft.nl',\n",
    "            'construction datetime': datetime.now().strftime(\"%d-%b-%Y (%H:%M:%S)\"),\n",
    "            'version': 'v1',\n",
    "            'version comments': 'constructed with xarray'}\n",
    "\n",
    "# Save the dataset to netCDF --------------------------------------------------------\n",
    "# First define a custom encoding dictionary for the ADV variables, to save variables with the same accuracy (same number of decimals) as the original text file with data\n",
    "encoding = {'p': { 'scale_factor': 10, 'dtype': 'int16', '_FillValue': -999, 'shuffle': False},\n",
    "            'u': { 'scale_factor': 0.001, 'dtype': 'int16', '_FillValue': -999, 'shuffle': False},  # three decimals originally, so scale factor 0.001. max value is 7m/s, with 3 decimals is 7000 options, so int16 scale of Â± 32767 is sufficient\n",
    "            'v': { 'scale_factor': 0.001, 'dtype': 'int16', '_FillValue': -999, 'shuffle': False},  # shuffle: flag for bit order. I just tried for which variables it saves data. (default is True when using deflate compression)\n",
    "            'w': { 'scale_factor': 0.001, 'dtype': 'int16', '_FillValue': -999, 'shuffle': False},\n",
    "            'anl1': { 'dtype': 'uint16'},\n",
    "            'anl2': { 'dtype': 'uint16'},\n",
    "            'a1': { 'dtype': 'int16', '_FillValue': -999},\n",
    "            'a2': { 'dtype': 'int16', '_FillValue': -999},\n",
    "            'a3': { 'dtype': 'int16', '_FillValue': -999},\n",
    "            'cor1': { 'dtype': 'int8', '_FillValue': -99},\n",
    "            'cor2': { 'dtype': 'int8', '_FillValue': -99},\n",
    "            'cor3': { 'dtype': 'int8', '_FillValue': -99},\n",
    "            'snr1': { 'scale_factor': 0.1, 'dtype': 'int16', '_FillValue': -999, 'shuffle': False},\n",
    "            'snr2': { 'scale_factor': 0.1, 'dtype': 'int16', '_FillValue': -999, 'shuffle': False},\n",
    "            'snr3': { 'scale_factor': 0.1, 'dtype': 'int16', '_FillValue': -999, 'shuffle': False},\n",
    "            'voltage': { 'scale_factor': 0.1, 'dtype': 'int16', '_FillValue': -999, 'shuffle': False},\n",
    "            'heading': { 'scale_factor': 0.1, 'dtype': 'int16', '_FillValue': -999},\n",
    "            'pitch': { 'scale_factor': 0.1, 'dtype': 'int16', '_FillValue': -999, 'shuffle': False},\n",
    "            'roll': { 'scale_factor': 0.1, 'dtype': 'int16', '_FillValue': -999, 'shuffle': False},\n",
    "            'burst': { 'scale_factor': 0.1, 'dtype': 'int16', '_FillValue': -999} }\n",
    "\n",
    "# Then add deflate compression level 4 to all variables and coordinates in netCDF, without overwriting existing keys\n",
    "compression = {var: {\"zlib\": True, \"complevel\": 4} for var in list(ds.data_vars) + list(ds.coords)}  # temporary dict, with only compression settings\n",
    "for var, comp in compression.items():  # for each variable in the dataset, \n",
    "    if var in encoding:                # if the variable already has an encoding, update it with the compression settings\n",
    "        encoding[var].update(comp)\n",
    "    else:                              # if the variable does not have an encoding yet, add it \n",
    "        encoding[var] = comp\n",
    "encoding\n",
    "\n",
    "ds.to_netcdf(os.path.join(ncOutDir, filename_out), encoding=encoding)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.p.plot()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lidar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
