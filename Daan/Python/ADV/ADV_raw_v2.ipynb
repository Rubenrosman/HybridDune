{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import sys\n",
    "sys.path.append(r'C:\\Users\\dpoppema\\OneDrive - Delft University of Technology\\Documents\\GitHub\\HybridDune\\Ruben\\ADV')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from vector import Vector\n",
    "from datetime import datetime\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data\n",
    "\n",
    "# Data loctions and filenames --------------------------------------------------------------------------------- \n",
    "# location of raw data\n",
    "dataFolder_all = [ r'O:\\HybridDune experiment\\data ADV, OBS\\ADV RWS1\\Deployment 1, until 23dec',\n",
    "                   r'O:\\HybridDune experiment\\data ADV, OBS\\ADV RWS2\\Deployment 1, until 23dec',\n",
    "                   r'O:\\HybridDune experiment\\data ADV, OBS\\ADV RWS3\\S3, Deployment1, until dec23',\n",
    "                   r'O:\\HybridDune experiment\\data ADV, OBS\\ADV RWS4\\Deployment until 23dec',\n",
    "                   r'O:\\HybridDune experiment\\data ADV, OBS\\ADV TUD10\\Deployment until 23dec',\n",
    "                   r'O:\\HybridDune experiment\\data ADV, OBS\\ADV RWS3\\Deployment 2, from 31dec' ]\n",
    "\n",
    "# name of the instantiated vector class. Will be used for saving file, and ...\n",
    "name_all = [ 'S1ADV1 raw data - period 1',\n",
    "             'S2ADV1 raw data - period 1',\n",
    "             'S3ADV1 raw data - period 1',\n",
    "             'S4ADV1 raw data - period 1',\n",
    "             'S3ADV2 raw data - period 1',\n",
    "             'S3ADV1 raw data - period 2' ]\n",
    "\n",
    "# start time over which to read data (must be larger than first recorded time)\n",
    "tstart_all = ['2024-12-13 09:00:00',  # Start date of all ADVs, deployment 1\n",
    "              '2024-12-13 09:00:00',\n",
    "              '2024-12-13 09:00:00',\n",
    "              '2024-12-13 09:00:00',\n",
    "              '2024-12-13 09:00:00',\n",
    "              '2024-12-31 11:00:00' ] # for deployment 2 of S3ADV2\n",
    "\n",
    "# stop time over which to read data (must be smaller than last recorded time)\n",
    "tstop_all = [ '2024-12-23 19:00:00', # last full hour of ADV\n",
    "              '2024-12-26 04:00:00', # last full hour of ADV (NB: some incomplete hours available after this time. empty battery?)\n",
    "              '2024-12-27 16:00:00', # last full hour of ADV  \n",
    "              '2024-12-27 17:00:00', # last availabe air pressure data (last full hour of ADV was 30-Dec 10:00, some incomplete hours afterwards)\n",
    "              '2024-12-23 04:00:00', # last full hour of ADV (NB: some incomplete hours available after this time. empty battery?)       \n",
    "              '2025-01-16 14:00:00' ] # last full hour of ADV, deployment 2\n",
    "t_installed_all = ['2024-12-17 12:00:00', # datetime that ADV was installed at location at the beach (time before this can be used to calibrate pressure sensors, but is otherwise useless)\n",
    "                   '2024-12-17 09:00:00',\n",
    "                   '2024-12-16 15:00:00',\n",
    "                   '2024-12-17 09:00:00',\n",
    "                   '2024-12-17 10:00:00',\n",
    "                   '2024-12-31 11:00:00' ] # Daan: 2 hour earlier possible. 9 hour start adv data, 10 hour first reliable air data, 11hour first exported air data (QC file)\n",
    "t_removed_all = [ '2024-12-23 10:00:00',\n",
    "                  '2024-12-23 19:00:00',\n",
    "                  '2024-12-23 21:00:00',\n",
    "                  '2024-12-23 21:00:00',\n",
    "                  '2024-12-23 04:00:00',\n",
    "                  '2025-01-14 09:00:00' ]  \n",
    "\n",
    "#filename_out = 'ADV_RWS4_Deployment1.nc'\n",
    "filename_out_all = [ 'ADV S1ADV1 raw data - period 1 20mins.nc',\n",
    "                     'ADV S2ADV1 raw data - period 1 20mins.nc',\n",
    "                     'ADV S3ADV1 raw data - period 1 20mins.nc',\n",
    "                     'ADV S4ADV1 raw data - period 1 20mins.nc',\n",
    "                     'ADV S3ADV2 raw data - period 1 20mins.nc',\n",
    "                     'ADV S3ADV1 raw data - period 2 20mins.nc' ] \n",
    "block_length = 20 #minutes. NB: BLOCK LENGTH CAN ONLY BE 20 OR 60 MINUTES, SPLITTIING IN BLOCKS IS HARDCODED FOR DEPLOYMENT PERIOD 1, GIVEN 10s GAP BETWEEN BURSTS\n",
    "\n",
    "# make map 'raw_netcdf' in dataFolder and set this as output directory\n",
    "ncOutDir = r'O:\\HybridDune experiment\\data ADV, OBS\\raw NetCDF'\n",
    "if not os.path.exists(ncOutDir):\n",
    "    os.makedirs(ncOutDir)\n",
    "\n",
    "# metadata: position/elevation sensor and bed ------------------------------------------------------------------------\n",
    "# x, y location in RD coordinates [m]\n",
    "xRD_all = [  72471.921,  72455.816,  72439.635,  72425.352,  72404.520, 72439.635] # S1 AdV 1-S4ADV1, S3ADV2 \n",
    "yRD_all = [ 452143.646, 452121.479, 452099.868, 452080.129, 452125.233, 452099.868]\n",
    "\n",
    "# bed level\n",
    "zb_i1 = np.array([          0.700,               0.633,                                    0.488,               0.488,              0.468             ]) # S1ADV1  [m NAP]\n",
    "zb_i2 = np.array([          0.673,               0.628,                                    0.551,               0.521,              0.451             ]) # S2ADV1\n",
    "zb_i3 = np.array([          0.665,               0.620,                                    0.675,               0.505,              0.437             ]) # S3ADV1\n",
    "zb_i4 = np.array([          0.671,               0.611,                                    0.631,               0.456,              0.376             ]) # S4ADV1\n",
    "zb_i5 = np.array([         -0.989,                                   -0.614,              -0.776,                                   -0.568            ]) # S3ADV2\n",
    "\n",
    "t_zb_i1 = pd.to_datetime([ '2024-12-17 11:30',  '2024-12-19 12:00',                       '2024-12-21 14:00',  '2024-12-22 15:30',  '2024-12-23 12:00']) \n",
    "t_zb_i2 = pd.to_datetime([ '2024-12-17 11:30',  '2024-12-19 12:00',                       '2024-12-21 14:00',  '2024-12-22 15:30',  '2024-12-23 12:00']) \n",
    "t_zb_i3 = pd.to_datetime([ '2024-12-17 11:30',  '2024-12-19 12:00',                       '2024-12-21 14:00',  '2024-12-22 15:30',  '2024-12-23 12:00']) \n",
    "t_zb_i4 = pd.to_datetime([ '2024-12-17 11:30',  '2024-12-19 12:00',                       '2024-12-21 14:00',  '2024-12-22 15:30',  '2024-12-23 12:00']) \n",
    "t_zb_i5 = pd.to_datetime([ '2024-12-17 11:30',                       '2024-12-20 12:00',  '2024-12-21 14:00',                       '2024-12-23 12:00']) \n",
    "\n",
    "zb_i6 = np.array([          0.647,              0.359,              0.549,              0.641,              0.535,              0.611            ]) # S3ADV1 deployment 2   \n",
    "t_zb_i6 = pd.to_datetime([ '2024-12-31 10:30', '2025-01-02 13:50', '2025-01-04 12:50', '2025-01-06 12:00', '2025-01-08 12:00', '2025-01-12 13:00'])\n",
    "\n",
    "# Instrument: elevation, orientation\n",
    "ti_i1 = pd.to_datetime([  '2024-12-17 11:30', '2024-12-21 14:00'                    ])  # datetime of zi, zi_p, zi_OBS and theta\n",
    "ti_i2 = pd.to_datetime([  '2024-12-17 11:30', '2024-12-21 14:00'                    ])  \n",
    "ti_i3 = pd.to_datetime([  '2024-12-17 11:30',                     '2024-12-22 15:30'])  \n",
    "ti_i4 = pd.to_datetime([  '2024-12-17 11:30', '2024-12-21 14:00', '2024-12-22 15:30'])  \n",
    "ti_i5 = pd.to_datetime([  '2024-12-17 11:30', '2024-12-21 14:00'])  \n",
    "\n",
    "zi_i1 = np.array([         0.953,              0.731                                ]) # NAP elevation of ADV measurement volume\n",
    "zi_i2 = np.array([         0.906,              0.794                                ])\n",
    "zi_i3 = np.array([         0.908,                                   0.750           ])\n",
    "zi_i4 = np.array([         0.924,              0.874,               0.699           ])\n",
    "zi_i5 = np.array([        -0.736,             -0.535                                ]) \n",
    "\n",
    "zi_OBShigh_i1 = np.array([ 0.995,              0.778                                ]) # NAP elevation of highest OBS sensor\n",
    "zi_OBShigh_i2 = np.array([ 0.953,              0.841                                ])\n",
    "zi_OBShigh_i3 = np.array([ 0.954,                                   0.797           ])\n",
    "zi_OBShigh_i4 = np.array([ 0.985,              0.921,               0.746           ])\n",
    "zi_OBShigh_i5 = np.array([-0.704,             -0.488                                ]) \n",
    "\n",
    "zi_p_i1      = zi_i1 + 0.157 + 0.22 # NAP elevation of pressure sensor (measurement volume is 15.7 cm below ADV head, pressure sensor 22 cm above head, if sensor vertical)\n",
    "zi_p_i2      = zi_i2 + 0.157 + 0.22\n",
    "zi_p_i3      = zi_i3 + 0.157 + 0.22\n",
    "zi_p_i4      = zi_i4 + 0.157 + 0.22\n",
    "zi_p_i5      = zi_i5 + 0.157 + 0.22\n",
    "\n",
    "zi_OBSlow_i1 = zi_OBShigh_i1 - 0.08 # for all sensors 8 cm below high OBS\n",
    "zi_OBSlow_i2 = zi_OBShigh_i2 - 0.08\n",
    "zi_OBSlow_i3 = zi_OBShigh_i3 - 0.08\n",
    "zi_OBSlow_i4 = zi_OBShigh_i4 - 0.08\n",
    "zi_OBSlow_i5 = zi_OBShigh_i5 - 0.08\n",
    "\n",
    "ti_i6           = pd.to_datetime(['2024-12-31 10:30', '2025-01-06 12:00'])\n",
    "zi_i6           = np.array([       1.047,              1.041            ])\n",
    "zi_OBShigh_i6   = np.array([       0.937,              0.931            ])\n",
    "zi_p_i6        = zi_i6 + 0.157 + 0.22\n",
    "zi_OBSlow_i6   = zi_OBShigh_i6 - 0.08\n",
    "\n",
    "# orientation of ADV (marked leg with respect to north, clockwise positive, in degrees)\n",
    "t_theta_i1 = pd.to_datetime([ '2024-12-17 09:00',  '2024-12-22 18:00'                                           ])  # time that ADV orientation was measured\n",
    "t_theta_i2 = pd.to_datetime([ '2024-12-17 09:00',  '2024-12-22 23:00',  '2024-12-23 07:00'                      ])\n",
    "t_theta_i3 = pd.to_datetime([ '2024-12-16 15:00',  '2024-12-22 15:21'                                           ])\n",
    "t_theta_i4 = pd.to_datetime([ '2024-12-17 09:00',  '2024-12-20 05:00',  '2024-12-21 11:00',  '2024-12-22 15:34' ])\n",
    "t_theta_i5 = pd.to_datetime([ '2024-12-17 10:00',  '2024-12-21 12:00',  '2024-12-22 11:00'                      ])\n",
    "\n",
    "theta_i1   = np.array([       304.9,               312.6                                                        ]) # degrees\n",
    "theta_i2   = np.array([       308.5,               314.5,               338.2                                   ])\n",
    "theta_i3   = np.array([       312.6,               304.8                                                        ])\n",
    "theta_i4   = np.array([       310.3,               315.7,               309.8,                306.2             ])\n",
    "theta_i5   = np.array([       306.3,               303.7,               307.5                                   ])\n",
    "\n",
    "t_theta_i6 = pd.to_datetime(['2024-12-31 10:30', '2025-01-06 12:00']) \n",
    "theta_i6   = np.array([       306.5,              304.7            ])\n",
    "\n",
    "# convert RD coordinates to local coordinates\n",
    "xy_RD = np.array([xRD_all, yRD_all]).T\n",
    "a = np.deg2rad(36)\n",
    "transformation_matrix = np.array([ [np.cos(a), np.sin(a)],[-np.sin(a), np.cos(a)] ])\n",
    "xy_loc = ( xy_RD - [71683.584, 452356.055] ) @ transformation_matrix\n",
    "x_loc_all = xy_loc.T[0]\n",
    "y_loc_all = xy_loc.T[1]\n",
    "\n",
    "# Metadata: rest ------------------------------------------------------------------------------------------------------\n",
    "serial_number_all = ['VEC13638',      'VEC14793',      'VEC14808',      'VEC13625',      'VEC13933',      'VEC14808'    ] \n",
    "OBS_type_all      = ['Cambell OBS3+', 'Cambell OBS3+', 'Cambell OBS3+', 'Cambell OBS3+', 'Seapoint STM', 'Cambell OBS3+'] \n",
    "offset_all        = [ 102718,          97849,           98378,           99549,           104195,          98378        ]  # offset to add to pressure sensor data to convert to absolute pressure [Pa]  #Daan: check last one\n",
    "\n",
    "# Check if block_length is 20 or 60 minutes\n",
    "if block_length not in [20, 60]:\n",
    "    raise ValueError('block_length must be 20 or 60 minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".dat file was read\n",
      ".sen file was read\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dpoppema\\OneDrive - Delft University of Technology\\Documents\\GitHub\\HybridDune\\Ruben\\ADV\\vector.py:330: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df3 = df3.fillna(method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".dat file was read\n",
      ".sen file was read\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dpoppema\\OneDrive - Delft University of Technology\\Documents\\GitHub\\HybridDune\\Ruben\\ADV\\vector.py:330: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df3 = df3.fillna(method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".dat file was read\n",
      ".sen file was read\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dpoppema\\OneDrive - Delft University of Technology\\Documents\\GitHub\\HybridDune\\Ruben\\ADV\\vector.py:330: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df3 = df3.fillna(method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".dat file was read\n",
      ".sen file was read\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dpoppema\\OneDrive - Delft University of Technology\\Documents\\GitHub\\HybridDune\\Ruben\\ADV\\vector.py:330: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df3 = df3.fillna(method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".dat file was read\n",
      ".sen file was read\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dpoppema\\OneDrive - Delft University of Technology\\Documents\\GitHub\\HybridDune\\Ruben\\ADV\\vector.py:330: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df3 = df3.fillna(method='ffill')\n",
      "C:\\Users\\dpoppema\\OneDrive - Delft University of Technology\\Documents\\GitHub\\HybridDune\\Ruben\\ADV\\vector.py:233: FutureWarning: Non-integer 'periods' in pd.date_range, pd.timedelta_range, pd.period_range, and pd.interval_range are deprecated and will raise in a future version.\n",
      "  timeDat = pd.date_range(start =self.tstart, periods = nSamples,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".dat file was read\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dpoppema\\OneDrive - Delft University of Technology\\Documents\\GitHub\\HybridDune\\Ruben\\ADV\\vector.py:288: FutureWarning: Non-integer 'periods' in pd.date_range, pd.timedelta_range, pd.period_range, and pd.interval_range are deprecated and will raise in a future version.\n",
      "  timeSen = pd.date_range(start = self.tstart, periods = nSamples,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".sen file was read\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dpoppema\\OneDrive - Delft University of Technology\\Documents\\GitHub\\HybridDune\\Ruben\\ADV\\vector.py:330: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df3 = df3.fillna(method='ffill')\n"
     ]
    }
   ],
   "source": [
    "for n_ADV in range(0,6):\n",
    "    # DEFINE FILE NAME, METADATA, ETC\n",
    "    # file name etc.  (tstart is the same for all ADVs, does not need to be defined in loop)\n",
    "    name          = name_all[n_ADV]\n",
    "    dataFolder    = dataFolder_all[n_ADV]\n",
    "    tstart        = tstart_all[n_ADV]\n",
    "    tstop         = tstop_all[n_ADV]\n",
    "    filename_out  = filename_out_all[n_ADV]\n",
    " \n",
    "    # Metadata\n",
    "    serial_number = serial_number_all[n_ADV]\n",
    "    OBS_type      = OBS_type_all[n_ADV]\n",
    "    offset        = offset_all[n_ADV]\n",
    "    xRD           = xRD_all[n_ADV]\n",
    "    yRD           = yRD_all[n_ADV]\n",
    "    x_loc         = x_loc_all[n_ADV] \n",
    "    y_loc         = y_loc_all[n_ADV]\n",
    "    t_installed = t_installed_all[n_ADV]  \n",
    "    t_removed = t_removed_all[n_ADV]\n",
    "\n",
    "    # Instrument, bed heights + corrresponding times \n",
    "    # Not the same length for all ADVs (eg sometimes measured twice, sometime 3 times), so defined separately in zi_i1, zi_i2 etc instead of in single matrix zb_all\n",
    "    zb            = eval('zb_i{}'.format(n_ADV+1))         # bed level [m NAP]. i+1 because n_ADV starts at 0, zb_i# at 1 \n",
    "    t_zb          = eval('t_zb_i{}'.format(n_ADV+1))\n",
    "    zi            = eval('zi_i{}'.format(n_ADV+1))         # NAP elevation of ADV measurement volume\n",
    "    t_zi          = eval('ti_i{}'.format(n_ADV+1))\n",
    "    zi_OBShigh    = eval('zi_OBShigh_i{}'.format(n_ADV+1)) # NAP elevation of highest OBS sensor, measured at same time as zi\n",
    "    zi_OBSlow     = eval('zi_OBSlow_i{}'.format(n_ADV+1))  # NAP elevation of lowest OBS sensor\n",
    "    zi_p          = eval('zi_p_i{}'.format(n_ADV+1))       # NAP elevation of pressure sensor \n",
    "    theta_ADV     = eval('theta_i{}'.format(n_ADV+1))      # orientation of ADV (marked leg with respect to north, clockwise positive, in degrees). NB: var name theta is reverved for data processing (dir bins)\n",
    "    t_theta       = eval('t_theta_i{}'.format(n_ADV+1))    # time that theta was measured\n",
    "\n",
    "    # IMPORT RAW DATA ---------------------------------------------------------------------------------\n",
    "    vec = Vector(name, dataFolder, tstart=tstart, tstop=tstop)\n",
    "\n",
    "    # reads the raw data from tstart to tstop and casts all data in a pandas DataFrame that is stored under vec.dfpuv.\n",
    "    # in case there is no data between tstart and tstop the DataFrame is not instantiated\n",
    "    vec.read_raw_data()\n",
    "\n",
    "    # break up the data into burst blocks\n",
    "    if n_ADV == 5:\n",
    "        vec.cast_to_blocks_in_xarray(blockWidth=block_length*60) # for deployment 2 of S3ADV1, measurements were continuous, no bursts. So casting in blocks is easier\n",
    "    else:\n",
    "        # NB: because data is measured in bursts, with 10s missing per hour, the function will always cast exactly in these burst, no matter what blockwidth is selected\n",
    "        vec.cast_to_blocks_in_xarray(blockWidth=3590) # 3590 s = 1 hr. data is always cast in blocks of 3590 seconds, whatever the value here, because the ADV measured in bursts  \n",
    "\n",
    "        # resample the blocks to 20  minutes instead of one hour, if asked for in value of block_length -------------\n",
    "        if block_length == 20: \n",
    "            # Extract the dataset from vec, for easier manipulation\n",
    "            ds = vec.ds\n",
    "\n",
    "            # select data of every t, all but the last N (number of samples per block not divisible by 3, so throw away last sample))\n",
    "            ds = ds.isel(N=slice(0, -1))\n",
    "\n",
    "            # make a variable t_new\n",
    "            t0 = ds['t'].values[0]  \n",
    "            N_t_new = 3 * ds.sizes['t'] # new number of blocks\n",
    "            N_N_new = 9573              # new number timesteps in block. (28720/3=9573.333, 1 timestep thrown away)\n",
    "\n",
    "            # make t_new: start at t0, with steps of [9573, 9573, 9654]/8 seconds (±20min, with 10s gap between bursts), and length of N_t_new\n",
    "            pattern = [9573, 9573, 9654]\n",
    "            repeats = N_t_new // len(pattern)\n",
    "            deltas = pattern * repeats \n",
    "            t_new = np.array([t0 + np.timedelta64(int(sum(deltas[:i])*1000/8), 'ms') for i in range(N_t_new)]) # *1000/8 to convert to ms, given 8hz\n",
    "            N_new = ds.N.values[0:N_N_new]\n",
    "\n",
    "            # make a new dataset ds_new, with the new time dimension and the new N dimension\n",
    "            ds_new = xr.Dataset( coords={ 't': t_new,\n",
    "                                        'N': N_new   })\n",
    "\n",
    "            # for every variable: reshape the values and copy the attributes\n",
    "            for var in ds.data_vars: \n",
    "                # if var has no coordinates, just copy it\n",
    "                if len(ds[var].dims) == 0:\n",
    "                    ds_new[var] = ds[var]\n",
    "                # Else (all others are t x N) reshape to (3*t) x (N/3)\n",
    "                else:\n",
    "                    arr = ds[var].values.reshape(N_t_new, N_N_new)\n",
    "                    ds_new[var] = (('t', 'N'), arr)\n",
    "                    # And for all vars, copy the attributes  \n",
    "                    ds_new[var].attrs = ds[var].attrs\n",
    "\n",
    "            # replace the dataset in the class instantiation by the new one, so that we can compute block averages\n",
    "            vec.ds = ds_new\n",
    "\n",
    "    # compute burst averages (make sure to read vector.py what is happening exactly!)\n",
    "    vec.compute_block_averages()\n",
    "\n",
    "    # all data is collected in an xarray Dataset ds. We extract this from the class instantiation and\n",
    "    # we can easily write it to netCDF\n",
    "    ds = vec.ds\n",
    "\n",
    "    # CALIBRATE PRESSURE ---------------------------------------------------------------------------------\n",
    "    ds.p.values[ds.p.values == 0] = np.nan       # make p nan where P=0: absolute pressure unknown: either be equal to offset or lower!\n",
    "    ds.p.values = ds.p.values + offset           # add offset for calibration, to obtain absolute pressure\n",
    "\n",
    "    # ADD METADATA: GLOBAL ATTRIBUTES AND METADATA VARIABLES ---------------------------------------------------\n",
    "    # Update pressure metadata\n",
    "    if n_ADV == 4: # S3ADV2 had a different calibration period and method: during the regular calibration period, the atmospheric pressure was below the minimum presure it could measure\n",
    "        cal_text = '{}'.format(offset) + ' Pa added to raw pressure to obtain absolute pressure, based on comparison with reference pressure sensor S3.P2 and S3.P3, for calibratinon period 17 Dec 2024, 16:00-18:00'\n",
    "\n",
    "    else:\n",
    "        cal_text = '{}'.format(offset) + ' Pa added to raw pressure to obtain absolute pressure, based on comparison with reference pressure sensor ref.P1 during the calibration period (16dec, 21:00-22:00, when ADV S1.ADV1-S4.ADV1 were all dry)'\n",
    "    ds.p.attrs.update({'comments': 'see variable zi_p for the NAP elevation of the pressure sensor', 'calibration': cal_text})\n",
    "\n",
    "    # Update OBS variables metadata\n",
    "    ds.anl1.attrs.update({'long_name': 'analog input 1: OBShigh', 'comment': 'unitless counts, 0-65535'})\n",
    "    ds.anl2.attrs.update({'long_name': 'analog input 2: OBSlow', 'comment': 'unitless counts, 0-65535'})\n",
    "\n",
    "    # Global attributes\n",
    "    ds.attrs = {'Conventions': 'CF-1.6',\n",
    "                'title': '{}'.format(vec.name),\n",
    "                'instrument': 'Nortek Vector',\n",
    "                'instrument serial number': serial_number,\n",
    "                'connected OBS type': OBS_type,\n",
    "                'time zone': 'UTC+1',\n",
    "                'coordinate type': 'XYZ',\n",
    "                'summary': 'hybrid-Dune field campaign',\n",
    "                'contact person': 'Daan Poppema',\n",
    "                'emailadres': 'd.w.poppema@tudelft.nl',\n",
    "                'construction datetime': datetime.now().strftime(\"%d-%b-%Y (%H:%M:%S)\"),\n",
    "                'version': 'v1',\n",
    "                'version comments': 'constructed with xarray'}\n",
    "    \n",
    "    # Add instrument variables for metadata (position etc are saved as variables, to make it easier to explain their meaning in the netCDF attributes)\n",
    "    # Scalar variables\n",
    "    ds['x_RD'] = xRD                                         # x position of instrument, in RDNAP coordinates [m]\n",
    "    ds['y_RD'] = yRD                                         # y position of instrument, in RDNAP coordinates [m]\n",
    "    ds['x_local'] = x_loc                                    # x position of instrument, in local coordinate system [m]\n",
    "    ds['y_local'] = y_loc                                    # y position of instrument, in local coordinate system [m] \n",
    "    ds['t_installed'] = t_installed                          # time that the instrument was installed at the indicated height and location at the beach \n",
    "    ds['t_removed'] = t_removed                              # time that the instrument was removed\n",
    "\n",
    "    # Matrix variables: position, orientation (measured at multiple times, so vectors instead of scalars) \n",
    "    # Syntax: first add the dimension, then assign the variable to the dimension\n",
    "    ds['t_theta'] = t_theta                              # time that theta was measured\n",
    "    ds['theta_ADV'] = ('t_theta',theta_ADV)              # orientation of ADV (marked leg with respect to north, clockwise positive, in degrees)\n",
    "    ds['t_zb'] = t_zb                                    # time that zb was measured\n",
    "    ds['zb'] = ('t_zb',zb)                               # bed level [m NAP]\n",
    "    ds['t_zi'] = t_zi                                    # time that sensor height was measured (measurement volume, pressure sensor and OBSs) \n",
    "    ds['zi'] = ('t_zi', zi)                              # NAP elevation of ADV measurement volume\n",
    "    ds['zi_OBShigh'] = ('t_zi',zi_OBShigh)               # NAP elevation of highest OBS sensor\n",
    "    ds['zi_OBSlow'] = ('t_zi',zi_OBSlow)                 # NAP elevation of lowest OBS sensor\n",
    "    ds['zi_p'] = ('t_zi',zi_p)                           # NAP elevation of pressure sensor\n",
    " \n",
    "    # Add attributes to metadata variables\n",
    "    local_coord_sys         = 'x=cross-shore (positive=landward); y=alongshore (positive is to north-east); (800,200) is the southern seaward corner of the containers'\n",
    "    coord_conv              = '(0,0) local is (71683.584,452356.055) RD coordinates; local x-axis is 36° clockwise from RD x-axis; i.e. [x_loc y_loc] = [x_RD y_RD] - [x0 y0] .* [cosd(36) sind(36); -sind(36) cosd(36)]'\n",
    "    ds.x_RD.attrs           = {'units': 'm', 'long_name': 'x position of instrument in RDNAP coordinates', 'epsg': 28992} # epsg: RD new\n",
    "    ds.y_RD.attrs           = {'units': 'm', 'long_name': 'y position of instrument in RDNAP coordinates', 'epsg': 28992}\n",
    "    ds.x_local.attrs        = {'units': 'm', 'long_name': 'cross-shore position of instrument in local coordinate system','local_coordinate_system': local_coord_sys, 'coordinate_conversion': coord_conv}\n",
    "    ds.y_local.attrs        = {'units': 'm', 'long_name': 'alongshore position of instrument in local coordinate system','local_coordinate_system': local_coord_sys, 'coordinate_conversion': coord_conv}\n",
    "    ds['t_installed'].attrs = {'long name': 'date and time that the instrument was installed at the indicated height and location at the beach'}\n",
    "    ds['t_removed'].attrs   = {'long name': 'date and time that the instrument was removed'}\n",
    "\n",
    "    ds.theta_ADV.attrs   = {'units': 'degrees', 'long_name': 'ADV orientation', 'definition': 'orientation of ADV x-pod with respect to north, clockwise positive', 'comment': 'measured manually with RTK GPS. For orientaton as continuously measured by vector, see heading, pitch, roll variables'}\n",
    "    ds.zb.attrs          = {'units': 'm +NAP', 'long_name': 'bed level'}  \n",
    "    ds.zi.attrs          = {'units': 'm +NAP', 'long_name': 'instrument elevation: ADV measurement volume'}  # instrument height\n",
    "    ds.zi_OBShigh.attrs  = {'units': 'm +NAP', 'long_name': 'instrument elevation: high OBS sensor'}\n",
    "    ds.zi_OBSlow.attrs   = {'units': 'm +NAP', 'long_name': 'instrument elevation: low OBS sensor'}\n",
    "    ds.zi_p.attrs        = {'units': 'm +NAP', 'long_name': 'instrument elevation: pressure sensor'}\n",
    "    ds.t_theta.attrs     = {'long name': 'time that instrument orientation was measured'}\n",
    "    ds.t_zb.attrs        = {'long name': 'time that bed level at instrument was measured'}\n",
    "    ds.t_zi.attrs        = {'long name': 'time that instrument elevation was measured (measurement volume, pressure sensor and OBSs)'}\n",
    "\n",
    "    # Save the dataset to netCDF --------------------------------------------------------\n",
    "    # For compression, define a custom encoding dictionary for the ADV variables, to save variables with the same accuracy (same number of decimals) as the \n",
    "    # original text file with data\n",
    "    encoding = {'p': { 'scale_factor': 10.0, 'dtype': 'uint16', '_FillValue': 0, 'add_offset': offset, 'shuffle': False}, #NB: scale factor 10.0, not 10, to make unpacked p float (able to contain NaN)\n",
    "                'u': { 'scale_factor': 0.001, 'dtype': 'int16', '_FillValue': -9999, 'shuffle': False},  # three decimals originally, so scale factor 0.001. max value is 7m/s, with 3 decimals is 7000 options, so int16 scale of ± 32767 is sufficient\n",
    "                'v': { 'scale_factor': 0.001, 'dtype': 'int16', '_FillValue': -9999, 'shuffle': False},  # shuffle: flag for bit order. I just tried for which variables it saves data. (default flag is True when using deflate compression)\n",
    "                'w': { 'scale_factor': 0.001, 'dtype': 'int16', '_FillValue': -9999, 'shuffle': False},\n",
    "                'anl1': { 'dtype': 'uint16', '_FillValue': 65535},                                       # generally no values equal to 65535 (sensors tend to almost clip, not actually clip). But file 1 has 6 obs equal to 65535 that will now be made nan\n",
    "                'anl2': { 'dtype': 'uint16', '_FillValue': 65535},\n",
    "                'a1': { 'dtype': 'int16', '_FillValue': -9999},\n",
    "                'a2': { 'dtype': 'int16', '_FillValue': -9999},\n",
    "                'a3': { 'dtype': 'int16', '_FillValue': -9999},\n",
    "                'cor1': { 'dtype': 'int8', '_FillValue': -99},\n",
    "                'cor2': { 'dtype': 'int8', '_FillValue': -99},\n",
    "                'cor3': { 'dtype': 'int8', '_FillValue': -99},\n",
    "                'snr1': { 'scale_factor': 0.1, 'dtype': 'int16', '_FillValue': -9999, 'shuffle': False},\n",
    "                'snr2': { 'scale_factor': 0.1, 'dtype': 'int16', '_FillValue': -9999, 'shuffle': False},\n",
    "                'snr3': { 'scale_factor': 0.1, 'dtype': 'int16', '_FillValue': -9999, 'shuffle': False},\n",
    "                'voltage': { 'scale_factor': 0.1, 'dtype': 'int16', '_FillValue': -9999, 'shuffle': False},\n",
    "                'heading': { 'scale_factor': 0.1, 'dtype': 'int16', '_FillValue': -9999},\n",
    "                'pitch': { 'scale_factor': 0.1, 'dtype': 'int16', '_FillValue': -9999, 'shuffle': False},\n",
    "                'roll': { 'scale_factor': 0.1, 'dtype': 'int16', '_FillValue': -9999, 'shuffle': False},\n",
    "                'burst': { 'scale_factor': 1.0, 'dtype': 'int16', '_FillValue': -9999} }\n",
    "    if n_ADV == 5: # for deployment 2 of S3ADV1, no bursts, so no burst variable\n",
    "        del encoding['burst']\n",
    "    # Then extend the dictionary: add deflate compression level 4 to all variables and coordinates in netCDF, without overwriting existing keys\n",
    "    compression = {var: {\"zlib\": True, \"complevel\": 4} for var in list(ds.data_vars) + list(ds.coords)}  # temporary dict, with only compression settings\n",
    "    for var, comp in compression.items():  # for each variable in the dataset, \n",
    "        if var in encoding:                # if the variable already has an encoding, update it with the compression settings\n",
    "            encoding[var].update(comp)\n",
    "        else:                              # if the variable does not have an encoding yet, add it \n",
    "            encoding[var] = comp\n",
    "    ds.encoding = encoding  # add the encoding to the dataset (not really necessary, but allows retrieval later on)\n",
    "\n",
    "    ds.to_netcdf(os.path.join(ncOutDir, filename_out), encoding=encoding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lidar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
