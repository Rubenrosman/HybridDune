{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "419c87ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script for importing netcdf, finding optimal shuffle settings for encoding, and then saving the dataset with these settings\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os   \n",
    "import copy\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873d0d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_file_in = r'O:\\HybridDune experiment\\2024-12-18 to 2024-12-20, Storm 1\\Lidars\\20241220_LiDAR1\\storm1_lidar1_polar_10sInterval - Copy.nc'\n",
    "map_file_out1 = r'O:\\HybridDune experiment\\2024-12-18 to 2024-12-20, Storm 1\\Lidars\\20241220_LiDAR1\\r no shuffle.nc'\n",
    "map_file_out2 = r'O:\\HybridDune experiment\\2024-12-18 to 2024-12-20, Storm 1\\Lidars\\20241220_LiDAR1\\r shuffle.nc'\n",
    "map_file_out3 = r'O:\\HybridDune experiment\\2024-12-18 to 2024-12-20, Storm 1\\Lidars\\20241220_LiDAR1\\i no shuffle.nc'\n",
    "map_file_out4 = r'O:\\HybridDune experiment\\2024-12-18 to 2024-12-20, Storm 1\\Lidars\\20241220_LiDAR1\\i shuffle.nc'\n",
    "map_file_out5 = r'O:\\HybridDune experiment\\2024-12-18 to 2024-12-20, Storm 1\\Lidars\\20241220_LiDAR1\\r_andi_i shuffle fillvalue_0.nc'\n",
    "\n",
    "ds = xr.open_dataset(map_file_in)\n",
    "# ds_r = ds.copy()\n",
    "# ds_i = ds.copy()\n",
    "\n",
    "# ds_r = ds_r[['radius_lidar']]  # keep only the 'r' variable\n",
    "# ds_i = ds_i[['intensity']]  # keep only the 'r' variable\n",
    "\n",
    "# # Radius\n",
    "# compression = {var: {\"zlib\": True, \"complevel\": 5} for var in list(ds_r.data_vars) + list(ds_r.coords)}  # temporary dict, with only compression settings\n",
    "\n",
    "# encoding = {'radius_lidar': { 'dtype': 'uint16', 'shuffle': False} }\n",
    "# for var, comp in compression.items():  # for each variable in the dataset, \n",
    "#     if var in encoding:                # if the variable already has an encoding, update it with the compression settings\n",
    "#         encoding[var].update(comp)\n",
    "#     else:                              # if the variable does not have an encoding yet, add it \n",
    "#         encoding[var] = comp\n",
    "# ds_r.to_netcdf(map_file_out1, encoding=encoding)  # Save the dataset with deflate compression\n",
    "\n",
    "# encoding = {'radius_lidar': { 'dtype': 'uint16', 'shuffle': True} }\n",
    "# for var, comp in compression.items():  # for each variable in the dataset, \n",
    "#     if var in encoding:                # if the variable already has an encoding, update it with the compression settings\n",
    "#         encoding[var].update(comp)\n",
    "#     else:                              # if the variable does not have an encoding yet, add it \n",
    "#         encoding[var] = comp\n",
    "# ds_r.to_netcdf(map_file_out2, encoding=encoding)  # Save the dataset with deflate compression\n",
    "\n",
    "\n",
    "# # Intensity ------------------------------\n",
    "# compression = {var: {\"zlib\": True, \"complevel\": 5} for var in list(ds_i.data_vars) + list(ds_i.coords)}  # temporary dict, with only compression settings\n",
    "\n",
    "# encoding = {'intensity': { 'dtype': 'uint8', 'shuffle': False} }\n",
    "# for var, comp in compression.items():  # for each variable in the dataset, \n",
    "#     if var in encoding:                # if the variable already has an encoding, update it with the compression settings\n",
    "#         encoding[var].update(comp)\n",
    "#     else:                              # if the variable does not have an encoding yet, add it \n",
    "#         encoding[var] = comp\n",
    "# ds_i.to_netcdf(map_file_out3, encoding=encoding)  # Save the dataset with deflate compression\n",
    "\n",
    "# encoding = {'intensity': { 'dtype': 'uint8', 'shuffle': True} }\n",
    "# for var, comp in compression.items():  # for each variable in the dataset, \n",
    "#     if var in encoding:                # if the variable already has an encoding, update it with the compression settings\n",
    "#         encoding[var].update(comp)\n",
    "#     else:                              # if the variable does not have an encoding yet, add it \n",
    "#         encoding[var] = comp\n",
    "# ds_i.to_netcdf(map_file_out4, encoding=encoding)  # Save the dataset with deflate compression\n",
    "\n",
    "# Conclusion: shuffle better\n",
    "# variable time_string from ds\n",
    "# ds = ds.drop_vars(['time_string'])  # drop the time_string variable, as it is not needed for the compression test\n",
    "\n",
    "compression = {var: {\"zlib\": True, \"complevel\": 5} for var in list(ds.data_vars) + list(ds.coords)}  # temporary dict, with only compression settings\n",
    "\n",
    "encoding = {'radius_lidar': { 'dtype': 'uint16', '_FillValue': 0, 'shuffle': True},\n",
    "            'intensity':    { 'dtype': 'uint8',  '_FillValue': 0, 'shuffle': True} }\n",
    "for var, comp in compression.items():  # for each variable in the dataset, \n",
    "    if var in encoding:                # if the variable already has an encoding, update it with the compression settings\n",
    "        encoding[var].update(comp)\n",
    "    else:                              # if the variable does not have an encoding yet, add it \n",
    "        encoding[var] = comp\n",
    "ds.to_netcdf(map_file_out5, encoding=encoding)  # Save the dataset with deflate compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca1e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_file_in = r'O:\\HybridDune experiment\\2024-12-18 to 2024-12-20, Storm 1\\Lidars\\20241220_LiDAR1\\storm1_lidar1_polar_10sInterval - Copy.nc'\n",
    "map_file_out1 = r'O:\\HybridDune experiment\\2024-12-18 to 2024-12-20, Storm 1\\Lidars\\20241220_LiDAR1\\r shuffle chunk_15240_240_1_1.nc'\n",
    "\n",
    "ds = xr.open_dataset(map_file_in)\n",
    "ds_r = ds.copy()\n",
    "ds_r = ds_r[['radius_lidar']]  # keep only the 'r' variable\n",
    "\n",
    "# Radius\n",
    "compression = {var: {\"zlib\": True, \"complevel\": 4} for var in list(ds_r.data_vars) + list(ds_r.coords)}  # temporary dict, with only compression settings\n",
    "\n",
    "encoding = {'radius_lidar': { 'dtype': 'uint16',  '_FillValue': 0, 'chunksizes': (1, 1, 240, 15240), 'shuffle': True} }\n",
    "for var, comp in compression.items():  # for each variable in the dataset, \n",
    "    if var in encoding:                # if the variable already has an encoding, update it with the compression settings\n",
    "        encoding[var].update(comp)\n",
    "    else:                              # if the variable does not have an encoding yet, add it \n",
    "        encoding[var] = comp\n",
    "ds_r.to_netcdf(map_file_out1, encoding=encoding)  # Save the dataset with deflate compression\n",
    "\n",
    "# Conclusion chunks: focus on time. 1 or 4 profiles does not matter. 240 or 720 angles does not matter. 240 angles better than 1. So \n",
    "# minimum total chunk size? And 5080 times is slightly better than 15240, so you can have too much time together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0b2e001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start saving file 1 at 2025-08-07 13:13:51.568585\n",
      "Saved dataset 1 at 2025-08-07 13:45:23.844315\n",
      "Saved dataset 2 at 2025-08-07 15:38:59.245318\n"
     ]
    }
   ],
   "source": [
    "#map_file_in = r'O:\\HybridDune experiment\\2024-12-18 to 2024-12-20, Storm 1\\Lidars\\20241220_LiDAR1\\storm1_lidar1_polar_10sInterval - Copy.nc'\n",
    "map_file_in = r'C:\\Matlab\\hybrid_dunes\\storm1_lidar1_polar - Copy.nc'\n",
    "map_file_out1 = r'O:\\HybridDune experiment\\2024-12-18 to 2024-12-20, Storm 1\\Lidars\\20241220_LiDAR1\\full_storm shuffle chunk_28800_180.nc'\n",
    "map_file_out2 = r'O:\\HybridDune experiment\\2024-12-18 to 2024-12-20, Storm 1\\Lidars\\20241220_LiDAR1\\full_storm shuffle chunk_28800_7200.nc'\n",
    "map_file_out3 = r'O:\\HybridDune experiment\\2024-12-18 to 2024-12-20, Storm 1\\Lidars\\20241220_LiDAR1\\r full_storm shuffle chunk_28800_120.nc'\n",
    "map_file_out4 = r'O:\\HybridDune experiment\\2024-12-18 to 2024-12-20, Storm 1\\Lidars\\20241220_LiDAR1\\r full_storm shuffle chunk_28800_90_1_3.nc'\n",
    "map_file_out5 = r'O:\\HybridDune experiment\\2024-12-18 to 2024-12-20, Storm 1\\Lidars\\20241220_LiDAR1\\r full_storm shuffle chunk_28800_30_1_3.nc'\n",
    "\n",
    "#ds = xr.open_dataset(map_file_in, chunks = \"{'time': 15240, 'profile_number': 1, 'obs_number': 90, 'echos': 1}\")  # load the dataset with specified chunking\n",
    "ds = xr.open_dataset(map_file_in, chunks = \"auto\")  # load the dataset with specified chunking\n",
    "#ds = xr.open_dataset(map_file_in)  # load the dataset with specified chunking\n",
    "\n",
    "#ds = ds[['radius_lidar']]  # keep only the 'r' variable\n",
    "\n",
    "# Radius\n",
    "compression = {var: {\"zlib\": True, \"complevel\": 4} for var in list(ds.data_vars) + list(ds.coords)}  # temporary dict, with only compression settings\n",
    "\n",
    "#encoding = {'radius_lidar': { 'dtype': 'uint16',  '_FillValue': 0, 'chunksizes': (1, 1, 240, 15240), 'shuffle': True} }\n",
    "#encoding = {'radius_lidar': { 'dtype': 'uint16', '_FillValue': 0, 'chunksizes': (1, 1, 180, 15240), 'shuffle': True},\n",
    "#            'intensity':    { 'dtype': 'uint8',  '_FillValue': 0, 'chunksizes': (1, 1, 180, 15240), 'shuffle': True} }\n",
    "encoding1 = {'radius_lidar': { 'dtype': 'uint16', '_FillValue': 0, 'chunksizes': (1, 1, 180, 28800), 'shuffle': True},\n",
    "             'intensity':    { 'dtype': 'uint8',  '_FillValue': 0, 'chunksizes': (1, 1, 180, 28800), 'shuffle': True} }\n",
    "encoding2 = {'radius_lidar': { 'dtype': 'uint16', '_FillValue': 0, 'chunksizes': (1, 1, 720, 28800), 'shuffle': True},\n",
    "             'intensity':    { 'dtype': 'uint8',  '_FillValue': 0, 'chunksizes': (1, 1, 720, 28800), 'shuffle': True} }\n",
    "encoding3 = {'radius_lidar': { 'dtype': 'uint16', '_FillValue': 0, 'chunksizes': (1, 1, 120, 28800), 'shuffle': True},\n",
    "             'intensity':    { 'dtype': 'uint8',  '_FillValue': 0, 'chunksizes': (1, 1, 120, 28800), 'shuffle': True} }\n",
    "encoding4 = {'radius_lidar': { 'dtype': 'uint16', '_FillValue': 0, 'chunksizes': (3, 1, 90, 28800), 'shuffle': True},\n",
    "             'intensity':    { 'dtype': 'uint8',  '_FillValue': 0, 'chunksizes': (3, 1, 90, 28800), 'shuffle': True} }\n",
    "encoding5 = {'radius_lidar': { 'dtype': 'uint16', '_FillValue': 0, 'chunksizes': (3, 1, 30, 28800), 'shuffle': True},\n",
    "             'intensity':    { 'dtype': 'uint8',  '_FillValue': 0, 'chunksizes': (3, 1, 30, 28800), 'shuffle': True} }\n",
    "\n",
    "for var, comp in compression.items():  # for each variable in the dataset, \n",
    "    if var in encoding1:                # if the variable already has an encoding, update it with the compression settings\n",
    "        encoding1[var].update(comp)\n",
    "    else:                              # if the variable does not have an encoding yet, add it \n",
    "        encoding1[var] = comp\n",
    "\n",
    "    if var in encoding2:                # if the variable already has an encoding, update it with the compression settings\n",
    "        encoding2[var].update(comp)\n",
    "    else:                              # if the variable does not have an encoding yet, add it \n",
    "        encoding2[var] = comp\n",
    "\n",
    "    if var in encoding3:                # if the variable already has an encoding, update it with the compression settings\n",
    "        encoding3[var].update(comp)\n",
    "    else:                              # if the variable does not have an encoding yet, add it \n",
    "        encoding3[var] = comp\n",
    "\n",
    "    if var in encoding4:                # if the variable already has an encoding, update it with the compression settings\n",
    "        encoding4[var].update(comp)\n",
    "    else:                              # if the variable does not have an encoding yet, add it \n",
    "        encoding4[var] = comp\n",
    "\n",
    "    if var in encoding5:                # if the variable already has an encoding, update it with the compression settings\n",
    "        encoding5[var].update(comp)\n",
    "    else:                              # if the variable does not have an encoding yet, add it \n",
    "        encoding5[var] = comp\n",
    "\n",
    "print(\"start saving file 1 at\", datetime.datetime.now()) # print  current time\n",
    "ds.to_netcdf(map_file_out1, encoding=encoding1, compute=True)  \n",
    "print(\"Saved dataset 1 at\", datetime.datetime.now()) \n",
    "\n",
    "ds.to_netcdf(map_file_out2, encoding=encoding2, compute=True)  # Save the dataset with deflate compression\n",
    "print(\"Saved dataset 2 at\", datetime.datetime.now()) \n",
    "\n",
    "# ds.to_netcdf(map_file_out3, encoding=encoding3, compute=True)  # Save the dataset with deflate compression\n",
    "# print(\"Saved dataset 3 at\", datetime.datetime.now()) \n",
    "\n",
    "# ds.to_netcdf(map_file_out4, encoding=encoding4, compute=True)  # Save the dataset with deflate compression\n",
    "# print(\"Saved dataset 4 at\", datetime.datetime.now())\n",
    "\n",
    "# ds.to_netcdf(map_file_out5, encoding=encoding5, compute=True)  # Save the dataset with deflate compression\n",
    "# print(\"Saved dataset 5 at\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5970e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 4GB\n",
      "Dimensions:         (time: 15240, profile_number: 16, echos: 3, obs_number: 720)\n",
      "Dimensions without coordinates: time, profile_number, echos, obs_number\n",
      "Data variables:\n",
      "    time_num        (time) datetime64[ns] 122kB dask.array<chunksize=(15240,), meta=np.ndarray>\n",
      "    time_string     (time) <U23 1MB dask.array<chunksize=(15240,), meta=np.ndarray>\n",
      "    file_name       (time) <U42 3MB dask.array<chunksize=(15240,), meta=np.ndarray>\n",
      "    profile_angle   (profile_number) float32 64B dask.array<chunksize=(16,), meta=np.ndarray>\n",
      "    ini_beam_angle  (profile_number) float32 64B dask.array<chunksize=(16,), meta=np.ndarray>\n",
      "    radius_lidar    (echos, profile_number, obs_number, time) float32 2GB dask.array<chunksize=(1, 4, 240, 5080), meta=np.ndarray>\n",
      "    intensity       (echos, profile_number, obs_number, time) float32 2GB dask.array<chunksize=(1, 6, 240, 5080), meta=np.ndarray>\n",
      "Attributes:\n",
      "    name:                   storm1_lidar1_polar polar\n",
      "    summary:                Hybrid Dune campaign, data of lidar 1 during storm 1\n",
      "    instrument:             lidar 1\n",
      "    period:                 storm 1, 2024-12-18 to 2024-12-20\n",
      "    instrument type:        Sick Multiscan 165\n",
      "    time zone:              UTC+1\n",
      "    contact person:         Daan Poppema\n",
      "    emailadres:             d.w.poppema@tudelft.nl\n",
      "    modification datetime:  26-Feb-2025 17:29:24\n",
      "    version:                v1\n",
      "    version comments:       \n"
     ]
    }
   ],
   "source": [
    "# To change\n",
    "# drop time_string\n",
    "# radius =double(radius)*1000, add scale factor\n",
    "# time_num: chunk\n",
    "# change metadata radius\n",
    "# try to add time coordinate, otherwise change metadata time_num\n",
    "# check first and last time: in open air?\n",
    "# make beam_angle 720*16, drop ini_beam_angle\n",
    "\n",
    "# To try\n",
    "# re-order dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "431da4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_file_in = r'O:\\HybridDune experiment\\2024-12-18 to 2024-12-20, Storm 1\\Lidars\\20241220_LiDAR1\\storm1_lidar1_polar - Copy.nc'\n",
    "map_file_out = r'O:\\HybridDune experiment\\2024-12-18 to 2024-12-20, Storm 1\\Lidars\\20241220_LiDAR1\\storm1_lidar1_polar - new.nc'\n",
    "\n",
    "ds = xr.open_dataset(map_file_in, chunks = \"auto\")  # load the dataset with specified chunking\n",
    "#print(ds.radius_lidar)\n",
    "\n",
    "# cast radius_lidar as float64\n",
    "radius_attrs = ds['radius_lidar'].attrs.copy()\n",
    "ds['radius_lidar'] = ds['radius_lidar'].astype('float64') / 1000 # convert to meters. Removes attributes, so reset/correct them below\n",
    "#ds['radius_lidar'].attrs = radius_attrs  # restore attributes\n",
    "ds['radius_lidar'].attrs = {'long name': 'distance (radius) from lidar to point', \n",
    "                            'units': 'm', \n",
    "                            'comment': 'radius, part of polar coordinates of points. Polar angles per point can be calculated from the profile_angle and beam_angle', \n",
    "                            'dimensions': 'T x 720 x 16 x 3 (Matlab) or reverse order (Python XArray), for number of point clouds x (360°x0.5° angular resolution) x 16 profiles x 3 echos. The lidar registers at most 3 echos at each angle, usually less.'}\n",
    "\n",
    "# rename coordinate time to t\n",
    "ds = ds.rename({'time': 't'})  # rename time coordinate to t\n",
    "ds['t'].attrs['comment'] = 'UTC+1: local wintertime'  # change long_name of t to time\n",
    "\n",
    "# assign values in time_num to coordinate of t\n",
    "ds = ds.assign_coords(t=ds.time_num.values)  # assign values in time_num to coordinate of t\n",
    "\n",
    "# make beam_angle instead of ini_beam_angle\n",
    "angles = np.arange(0, 360, 0.5)  # create angles\n",
    "ini_beam_angle = ds['ini_beam_angle'].values  # get ini_beam_angle values. 1x16 matrix\n",
    "beam_angle = np.repeat(ini_beam_angle[np.newaxis, :], 720, axis=0)  # repeat ini_beam_angle 720 times, for each angle. gives 720x16 matrix\n",
    "beam_angle = beam_angle + np.repeat(angles[:, np.newaxis], 16, axis=1)  # repeat angles for each profile, gives 720x16 matrix\n",
    "\n",
    "\n",
    "ds['beam_angle'] = (('profile_number', 'obs_number'), beam_angle.T)  # add beam_angle as a new coordinate\n",
    "ds['beam_angle'].attrs = {'long name': 'angle of the lidar beam', \n",
    "                          'units': 'degrees', \n",
    "                          'comment': 'the angular resolution is 0.5°. So every next measurement within a profile is exactly 0.5 degrees later. The angle of the first point differs slightly between profiles. Angles are identical between epochs (i.e. over time).'}\n",
    "\n",
    "# Drop variables: time_string, time_num, ini_beam_angle\n",
    "ds = ds.drop_vars('time_string')     # drop time_string variable\n",
    "ds = ds.drop_vars('time_num')        # drop time_num variable\n",
    "ds = ds.drop_vars('ini_beam_angle')  # drop ini_beam_angle variable\n",
    "#ds = ds[['beam_angle']]  # keep only the 'r' variable\n",
    "\n",
    "# Radius\n",
    "compression = {var: {\"zlib\": True, \"complevel\": 4} for var in list(ds.data_vars) + list(ds.coords)}  # temporary dict, with only compression settings\n",
    "encoding = {'radius_lidar': { 'scale_factor': 0.001, 'dtype': 'uint16', '_FillValue': 0, 'chunksizes': (1, 1, 120, 28800)},\n",
    "             'intensity':   {                        'dtype': 'uint8',  '_FillValue': 0, 'chunksizes': (1, 1, 120, 28800)} }  # encoding for the dataset  \n",
    "\n",
    "for var, comp in compression.items():  # for each variable in the dataset, \n",
    "    if var in encoding:                # if the variable already has an encoding, update it with the compression settings\n",
    "        encoding[var].update(comp)\n",
    "    else:                              # if the variable does not have an encoding yet, add it \n",
    "        encoding[var] = comp\n",
    "\n",
    "ds.to_netcdf(map_file_out, encoding=encoding, compute=True)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lidar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
