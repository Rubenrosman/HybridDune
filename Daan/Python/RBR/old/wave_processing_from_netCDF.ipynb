{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "# sys.path.append(r'C:\\Users\\ruro\\OneDrive - Boskalis\\Documents\\python\\OSSI')\n",
    "from scipy.signal import welch\n",
    "sys.path.append(r'C:\\Users\\dpoppema\\Documents\\GitHub\\HybridDune\\Ruben\\Pressure_sensors\\S1\\RBR_05')\n",
    "import puv \n",
    "#from cdo import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LOAD RAW DATA\n",
    "# Physical constants -----------------------------------------------------------------------------------\n",
    "rho = 1027 #kg/m3, for seawater at 9C, avg temp at HKZ measurement station\n",
    "g = 9.8125  # value Zandmotor\n",
    "\n",
    "# # input parameters per file ----------------------------------\n",
    "subfolder_in_all = ['refP1 RBR4','S1P2 RBR3', 'S1P3 RBR5', 'S2P3 RBR1', 'S3P3 RBR6', 'S4P3 RBR2'] # subfolder where file is sitting within experimentFolder (on O drive Daan)\n",
    "instrumentname_all = subfolder_in_all #['refP1 RBR4', 'S3P6 RBR6']\n",
    "sf_all = [8, 8, 16, 8, 16, 8] #[hz] sampling frequency\n",
    "\n",
    "# # input parameters general ---------------------------------------------\n",
    "experimentFolder = r'O:\\HybridDune experiment\\data RBR, OSSI\\copy RBR Udrive series1'                  # path where the data is sitting # Rubens Laptop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp.P.plot()\n",
    "start_time = pd.Timestamp(\"2024-12-13T16:00:00\")\n",
    "end_time = pd.Timestamp(\"2024-12-27T13:00:00\")\n",
    "plt.xlim(start_time, end_time)\n",
    "pt\n",
    "# plt.xlim(ds0.t.min(), ds0.t.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds0.p.plot()\n",
    "ds0.pc.plot()\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,2))\n",
    "ds0.pc.plot()\n",
    "plt.grid()\n",
    "\n",
    "start_time = pd.Timestamp(\"2024-12-17T16:00:00\")\n",
    "end_time = pd.Timestamp(\"2024-12-23T13:00:00\")\n",
    "plt.xlim(start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,2))\n",
    "ds0.pc.plot()\n",
    "plt.grid()\n",
    "\n",
    "start_time = pd.Timestamp(\"2024-12-17T20:00:00\")\n",
    "end_time = pd.Timestamp(\"2024-12-18T13:00:00\")\n",
    "plt.xlim(start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,2))\n",
    "ds0.pc.plot()\n",
    "plt.grid()\n",
    "\n",
    "start_time = pd.Timestamp(\"2024-12-17T20:00:00\")\n",
    "end_time = pd.Timestamp(\"2024-12-17T20:05:00\")\n",
    "plt.xlim(start_time, end_time)\n",
    "plt.ylim(-100, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "i = 4\n",
    "# frequency resolution in fourier space\n",
    "### delta_f = p_blocks/D_timeframe ###\n",
    "p_blocks = 30   # number of segments within block, for the Welch method\n",
    "D_length = 1800 # Duration of block in seconds (30 minutes)\n",
    "fresolution = p_blocks / D_length # Frequency resolution is 1/T_segment = n_segments / T_block\n",
    "nperseg = D_length * sf_all[i] / p_blocks - 0.5 #dim should be len(ds.f); whelch has (nperseg/2 +1)\n",
    "instrument = instrumentname_all[i]  # select instrument\n",
    "\n",
    "instrFile = os.path.join(experimentFolder,'QC', instrument +' p_rel.nc')\n",
    "ncOutFile = os.path.join(experimentFolder,'tailored', instrument +'.nc')\n",
    "\n",
    "\n",
    "# %% load the raw data from netcdf\n",
    "ds0 = xr.open_dataset(instrFile)\n",
    "print(np.size(ds0.p.values), 'time steps in dataset')\n",
    "\n",
    "#let's remove the bursts where there are only nans\n",
    "#ds0 = ds0.dropna(dim='t')                                      # temp skipped\n",
    "\n",
    "# make a new dataset that has an extra dimension to accomodate for the frequency axis\n",
    "ds = xr.Dataset(data_vars={},\n",
    "                coords={'t': ds0.t.values,\n",
    "                        'N': ds0.N.values,\n",
    "                        'f': np.arange(0, ds0.sf.values / 2, fresolution)})\n",
    "ds['f'].attrs = {'units': 'Hz'}\n",
    "ds.attrs = ds0.attrs\n",
    "\n",
    "# put all variables in this new dataset\n",
    "for key in ds0.data_vars:\n",
    "    ds[key] = ds0[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19200\n"
     ]
    }
   ],
   "source": [
    "print(np.size(ds.N))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 248MB\n",
      "Dimensions:  (t_full: 21158401, t: 256, N: 19200, f: 480)\n",
      "Coordinates:\n",
      "  * t_full   (t_full) datetime64[ns] 169MB 2024-12-12T09:00:00 ... 2024-12-27...\n",
      "  * t        (t) datetime64[ns] 2kB 2024-12-17T14:20:00 ... 2024-12-23T10:40:00\n",
      "  * N        (N) float64 154kB 0.0 0.0625 0.125 ... 1.2e+03 1.2e+03 1.2e+03\n",
      "  * f        (f) float64 4kB 0.0 0.01667 0.03333 0.05 ... 7.933 7.95 7.967 7.983\n",
      "Data variables:\n",
      "    p        (t, N) float64 39MB 6.616e+03 6.548e+03 ... 1.357e+04 1.357e+04\n",
      "    zi       float64 8B ...\n",
      "    zb       float64 8B ...\n",
      "    sf       int64 8B 16\n",
      "    p_rel    (t, N) float64 39MB 6.616e+03 6.548e+03 ... 1.357e+04 1.357e+04\n",
      "    name     <U9 36B ...\n",
      "4915200 size p\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 21158401 into shape (256,19200)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m burstLength \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m19200\u001b[39m  \u001b[38;5;66;03m# number of samples per burst\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m#ds_new['t_2d'] = (('t', 'N'), t_full.reshape((nBursts, burstLength)))      # relative pressure, pAir subtracted\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m ds_new[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt_2d\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnBursts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mburstLength\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)      \u001b[38;5;66;03m# relative pressure, pAir subtracted\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# reshape \u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m#ds_new.p2 = ds_new.t_2d.stack(z=('t_full', 'N')).reset_index('z')\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m#ds_new.p2.attrs = ds_new.p.attrs\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m#ds_new.p2.name = 'p2'\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# save reshaped\u001b[39;00m\n\u001b[0;32m     48\u001b[0m ncFilePath5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mHybridDune experiment\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata RBR, OSSI\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcopy RBR Udrive series1\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mQC\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcopy with t_full reshape.nc\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 21158401 into shape (256,19200)"
     ]
    }
   ],
   "source": [
    "# temp: resize p -----------------------------------------------------\n",
    "\n",
    "# Create a continuous time array\n",
    "# Make full time vector\n",
    "t0 = datetime(2024, 12, 12, 9, 0, 0)\n",
    "t_end = datetime(2024, 12, 27, 16, 20, 0)\n",
    "t_full   = pd.date_range(t0, t_end, freq='{}s'.format(1 / 16)) # 16hz time vector\n",
    "x = np.arange(0, len(t_full))  # create an index for the full time vector;\n",
    "\n",
    "ds_new = xr.Dataset(data_vars={},\n",
    "                coords={'t_full': t_full,\n",
    "                        't': ds0.t.values,\n",
    "                        'N': ds0.N.values,\n",
    "                        'f': np.arange(0, ds0.sf.values / 2, fresolution)})\n",
    "\n",
    "# put all variables in this new dataset\n",
    "for key in ds.data_vars:\n",
    "    ds_new[key] = ds[key]\n",
    "\n",
    "print(ds_new)\n",
    "print(np.size(ds_new.p.values), 'size p')\n",
    "\n",
    "# Save with compression\n",
    "ncFilePath1 = r'O:\\HybridDune experiment\\data RBR, OSSI\\copy RBR Udrive series1\\QC\\copy.nc'\n",
    "ncFilePath2 = r'O:\\HybridDune experiment\\data RBR, OSSI\\copy RBR Udrive series1\\QC\\copy with t_full.nc'\n",
    "ncFilePath3 = r'O:\\HybridDune experiment\\data RBR, OSSI\\copy RBR Udrive series1\\QC\\copy deflate4.nc'\n",
    "ncFilePath4 = r'O:\\HybridDune experiment\\data RBR, OSSI\\copy RBR Udrive series1\\QC\\copy with t_full deflate4.nc'\n",
    "\n",
    "encoding3 = {var: {\"zlib\": True, \"complevel\": 4} for var in ds.data_vars}\n",
    "encoding4 = {var: {\"zlib\": True, \"complevel\": 4} for var in ds_new.data_vars}\n",
    "\n",
    "\n",
    "#ds.to_netcdf(    ncFilePath1)\n",
    "#ds_new.to_netcdf(ncFilePath2)\n",
    "#ds.to_netcdf(    ncFilePath3, encoding=encoding3)\n",
    "#ds_new.to_netcdf(ncFilePath4, encoding=encoding4)\n",
    "nBursts = 256\n",
    "burstLength = 19200  # number of samples per burst\n",
    "#ds_new['t_2d'] = (('t', 'N'), t_full.reshape((nBursts, burstLength)))      # relative pressure, pAir subtracted\n",
    "ds_new['t_2d'] = (('t', 'N'), x.reshape((nBursts, burstLength)))      # relative pressure, pAir subtracted\n",
    "\n",
    "# reshape \n",
    "#ds_new.p2 = ds_new.t_2d.stack(z=('t_full', 'N')).reset_index('z')\n",
    "#ds_new.p2.attrs = ds_new.p.attrs\n",
    "#ds_new.p2.name = 'p2'\n",
    "\n",
    "# save reshaped\n",
    "ncFilePath5 = r'O:\\HybridDune experiment\\data RBR, OSSI\\copy RBR Udrive series1\\QC\\copy with t_full reshape.nc'\n",
    "ncFilePath6 = r'O:\\HybridDune experiment\\data RBR, OSSI\\copy RBR Udrive series1\\QC\\copy with t_full reshape deflate4.nc'\n",
    "encoding6 = {var: {\"zlib\": True, \"complevel\": 4} for var in ds_new.data_vars}\n",
    "#ds_new.to_netcdf(ncFilePath5)\n",
    "#ds_new.to_netcdf(ncFilePath6, encoding=encoding6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256.0\n",
      "21158401\n"
     ]
    }
   ],
   "source": [
    "print(np.size(ds.p)/19200)\n",
    "nperseg = D_length * sf_all[i] / p_blocks - 0.5 #dim should be len(ds.f); whelch has (nperseg/2 +1)\n",
    "a = np.arange(0,5)\n",
    "print(len(t_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DAAN: REMOVE FILTERING WITH OWN VERSION !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "def remove_outliers(data, threshold=3):\n",
    "    mean = np.mean(data)\n",
    "    std_dev = np.std(data)\n",
    "    mask = np.abs(data - mean) < threshold * std_dev\n",
    "    # filtered_data = np.where(mask, data, mean + threshold * std_dev * np.sign(data - mean))  # Replace outliers with closest value within threshold\n",
    "    filtered_data = np.where(mask, data, np.nan) # Replace outliers with nan\n",
    "    return filtered_data\n",
    "\n",
    "# Create a lambda function to pass the threshold parameter\n",
    "ufunc = lambda x: remove_outliers(x, threshold=3)\n",
    "\n",
    "# Apply the outlier removal function to the pressure data\n",
    "ds['p'] = xr.apply_ufunc(ufunc, \n",
    "                          ds['p'],\n",
    "                          input_core_dims=[['N']],\n",
    "                          output_core_dims=[['N']],\n",
    "                          vectorize=True)\n",
    "\n",
    "# Interpolate NaN values in the pressure signal\n",
    "ds['p'] = ds['p'].interpolate_na(dim='N', method='linear')\n",
    "\n",
    "\n",
    "ds.p.isel(t=111).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the initial number of NaN values\n",
    "initial_nan_count = np.sum(np.isnan(ds['p'].values))\n",
    "print(\"Initial NaN count:\", initial_nan_count)\n",
    "\n",
    "# Interpolating NaN values using linear interpolation\n",
    "ds['p'] = ds['p'].interpolate_na(dim='N', method='linear')\n",
    "\n",
    "# Handling any remaining NaN values\n",
    "ds['p'] = ds['p'].ffill(dim='N').bfill(dim='N')\n",
    "\n",
    "# Print the number of NaN values after interpolation\n",
    "remaining_nan_count = np.sum(np.isnan(ds['p'].values))\n",
    "print(\"Remaining NaN count:\", remaining_nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pressure filtering ####\n",
    "ufunc = lambda x: puv.band_pass_filter2(ds.sf.values, x, fmin=0.004, fmax=1.5)\n",
    "\n",
    "ds['p'] = xr.apply_ufunc(ufunc, \n",
    "                          ds['p'],\n",
    "                          input_core_dims=[['N']],\n",
    "                          output_core_dims=[['N']],\n",
    "                          vectorize=True)\n",
    "\n",
    "# Compute water depth\n",
    "ds['h'] = (ds['p'] / (rho * g) + (ds['zi'] - ds['zb'])).mean(dim='N')  # Relative to bed level\n",
    "ds['h'].attrs = {'long_name': 'mean water level', 'units': '[m] above bed level'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( np.size(ds.p.values) /19200 )\n",
    "print( np.size(ds.h.values) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several wave statistics computations, only based on pressure, for full bandpass\n",
    "Threshold= 4    # DAAN: SKIP\n",
    "\n",
    "\n",
    "ufunc = lambda x, h: puv.attenuate_signal(\n",
    "    'pressure', \n",
    "    ds.sf.values, x, h, \n",
    "    ds.zi.values,\n",
    "    ds.zb.values,\n",
    "    rho=1027,\n",
    "    g=9.8125,\n",
    "    removeNoise=False,\n",
    "    detrend=True)\n",
    "\n",
    "fx, ds['zs'] = xr.apply_ufunc(ufunc, \n",
    "                              ds['p'], ds['h'],\n",
    "                              input_core_dims=[['N'], []],\n",
    "                              output_core_dims=[['f'], ['N']],\n",
    "                              vectorize=True)\n",
    "\n",
    "ufunc = lambda zs: remove_outliers(zs, threshold=Threshold)   # DAAN: SKIP\n",
    "\n",
    "# ds['zs'] = xr.apply_ufunc(ufunc, \n",
    "#                           ds['zs'],\n",
    "#                           input_core_dims=[['N']],\n",
    "#                           output_core_dims=[['N']],\n",
    "#                           vectorize=True)\n",
    "\n",
    "# # Interpolate NaN values in the pressure signal\n",
    "# ds['zs'] = ds['zs'].interpolate_na(dim='N', method='linear')\n",
    "\n",
    "# Drop values that are below bed level. \n",
    "# ds['zs'] = ds['zs'].where(ds['zs'] >= 0.0, drop=True)\n",
    "\n",
    "ds['zs'].attrs = {'units': 'm', 'long_name': 'surface elevation'}   # DAAN: CHECK: MORE METADATA NEEDED?\n",
    "\n",
    "ufunc = lambda p: welch(p, fs=ds.sf.values, nperseg=nperseg, detrend='constant', window='hann') # Detrend: is per segment. 1min, about constant, so false used\n",
    "\n",
    "ds['frequencies'], ds['psd'] = xr.apply_ufunc(ufunc,\n",
    "                                                ds['zs'],\n",
    "                                                input_core_dims=[['N']],\n",
    "                                                output_core_dims=[['f'], ['f']],\n",
    "                                                vectorize=True)\n",
    "\n",
    "ufunc = lambda psd: puv.compute_wave_params(ds.f.values, psd, fmin=0.004 , fmax=5)   # DAAN: CHECK: WHY fmax=5? Different result than 1.5?\n",
    "\n",
    "ds['Hm0'], ds['Tp'], ds['Tm01'], ds['Tm02'], ds['Tmm10'], ds['Tps'] = xr.apply_ufunc(ufunc,\n",
    "                                                                          ds['psd'],\n",
    "                                                                          input_core_dims=[['f']],\n",
    "                                                                          output_core_dims=[[], [], [], [], [], []],\n",
    "                                                                          vectorize=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## low frequencies: IG waves ##\n",
    "## now first filter bandpass, then compute wave_params\n",
    "\n",
    "ufunc = lambda x: puv.band_pass_filter2(ds.sf.values, x, fmin=0.004, fmax=0.05)\n",
    "\n",
    "ds['p_low'] = xr.apply_ufunc(ufunc, \n",
    "                          ds['p'],\n",
    "                          input_core_dims=[['N']],\n",
    "                          output_core_dims=[['N']],\n",
    "                          vectorize=True)\n",
    "\n",
    "ufunc = lambda x, h: puv.attenuate_signal(\n",
    "    'pressure', \n",
    "    ds.sf.values, x, h, \n",
    "    ds.zi.values,\n",
    "    ds.zb.values,\n",
    "    rho=1027,\n",
    "    g=9.8125,\n",
    "    removeNoise=False,\n",
    "    detrend=True)\n",
    "\n",
    "fx, ds['zs_low'] = xr.apply_ufunc(ufunc, \n",
    "                              ds['p_low'], ds['h'],\n",
    "                              input_core_dims=[['N'], []],\n",
    "                              output_core_dims=[['f'], ['N']],\n",
    "                              vectorize=True)\n",
    "\n",
    "# ufunc = lambda zs: remove_outliers(zs, threshold=Threshold)\n",
    "\n",
    "# ds['zs_low'] = xr.apply_ufunc(ufunc, \n",
    "#                           ds['zs_low'],\n",
    "#                           input_core_dims=[['N']],\n",
    "#                           output_core_dims=[['N']],\n",
    "#                           vectorize=True)\n",
    "\n",
    "# ds['zs_low'] = ds['zs_low'].interpolate_na(dim='N', method='linear')\n",
    "\n",
    "# ds['zs_low'] = ds['zs_low'].where(ds['zs_low'] >= 0.0, drop=True)\n",
    "\n",
    "ds['zs_low'].attrs = {'units': 'm', 'long_name': 'surface elevation low freq.'}\n",
    "\n",
    "\n",
    "ufunc = lambda p: welch(p, fs=ds.sf.values, nperseg=nperseg, detrend='constant', window='hann')\n",
    "\n",
    "ds['frequencies_low'], ds['psd_low'] = xr.apply_ufunc(ufunc,\n",
    "                                                ds['zs_low'],\n",
    "                                                input_core_dims=[['N']],\n",
    "                                                output_core_dims=[['f'], ['f']],\n",
    "                                                vectorize=True)\n",
    "\n",
    "ufunc = lambda psd: puv.compute_wave_params(ds.f.values, psd, fmin=0.004 , fmax=0.05)\n",
    "\n",
    "ds['Hm0_low'], ds['Tp_low'], ds['Tm01_low'], ds['Tm02_low'], ds['Tmm10_low'], ds['Tps_low'] = xr.apply_ufunc(ufunc,\n",
    "                                                                          ds['psd_low'],\n",
    "                                                                          input_core_dims=[['f']],\n",
    "                                                                          output_core_dims=[[], [], [], [], [], []],\n",
    "                                                                          vectorize=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## High frequencies: wind-waves ##\n",
    "## now first filter bandpass, then compute wave_params\n",
    "\n",
    "ufunc = lambda x: puv.band_pass_filter2(ds.sf.values, x, fmin=0.05, fmax=5)\n",
    "\n",
    "ds['p_high'] = xr.apply_ufunc(ufunc, \n",
    "                          ds['p'],\n",
    "                          input_core_dims=[['N']],\n",
    "                          output_core_dims=[['N']],\n",
    "                          vectorize=True)\n",
    "\n",
    "ufunc = lambda x, h: puv.attenuate_signal(\n",
    "    'pressure', \n",
    "    ds.sf.values, x, h, \n",
    "    ds.zi.values,\n",
    "    ds.zb.values,\n",
    "    rho=1027,\n",
    "    g=9.8125,\n",
    "    removeNoise=False,\n",
    "    detrend=True)\n",
    "\n",
    "t, ds['zs_high'] = xr.apply_ufunc(ufunc, \n",
    "                              ds['p_high'], ds['h'],\n",
    "                              input_core_dims=[['N'], []],\n",
    "                              output_core_dims=[['f'], ['N']],\n",
    "                              vectorize=True)\n",
    "\n",
    "ufunc = lambda zs: remove_outliers(zs, threshold=Threshold)\n",
    "\n",
    "# ds['zs_high'] = xr.apply_ufunc(ufunc, \n",
    "#                           ds['zs_high'],\n",
    "#                           input_core_dims=[['N']],\n",
    "#                           output_core_dims=[['N']],\n",
    "#                           vectorize=True)\n",
    "\n",
    "# ds['zs_high'] = ds['zs_high'].interpolate_na(dim='N', method='linear')\n",
    "\n",
    "# ds['zs_high'] = ds['zs_high'].where(ds['zs_high'] >= 0.0, drop=True)\n",
    "\n",
    "ds['zs_high'].attrs = {'units': 'm', 'long_name': 'surface elevation high freq.'}\n",
    "\n",
    "\n",
    "\n",
    "ufunc = lambda p: welch(p, fs=ds.sf.values, nperseg=nperseg, detrend='constant', window='hann')\n",
    "\n",
    "ds['frequencies_high'], ds['psd_high'] = xr.apply_ufunc(ufunc,\n",
    "                                                ds['zs_high'],\n",
    "                                                input_core_dims=[['N']],\n",
    "                                                output_core_dims=[['f'], ['f']],\n",
    "                                                vectorize=True)\n",
    "\n",
    "ufunc = lambda psd: puv.compute_wave_params(ds.f.values, psd, fmin=0.05 , fmax=5)\n",
    "\n",
    "ds['Hm0_high'], ds['Tp_high'], ds['Tm01_high'], ds['Tm02_high'], ds['Tmm10_high'], ds['Tps_high'] = xr.apply_ufunc(ufunc,\n",
    "                                                                          ds['psd_high'],\n",
    "                                                                          input_core_dims=[['f']],\n",
    "                                                                          output_core_dims=[[], [], [], [], [], []],\n",
    "                                                                          vectorize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## skewness of waves ##\n",
    "ufunc = lambda p: puv.compute_SkAs(ds.sf.values,p,fpfac =None, fbounds = None)   \n",
    "\n",
    "ds['Sk'], ds['As'], ds['sig'] =  xr.apply_ufunc(ufunc,\n",
    "                                                ds['p'], \n",
    "                                                input_core_dims=[['N']],\n",
    "                                                output_core_dims=[[], [], []],\n",
    "                                                vectorize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten all variables with dimension 'N'\n",
    "# for var in ds.data_vars:\n",
    "#     if 'N' in ds[var].dims:\n",
    "#         ds[var] = ds[var].stack(t_N=('t', 'N'))\n",
    "\n",
    "# %% write to file\n",
    "# we strip all information on burst scale from the dataset to reduce size (and this info is already present in the raw_netcdf version of the data)\n",
    "# dsTailored = ds.drop_dims('N')\n",
    "dsTailored = ds\n",
    "\n",
    "\n",
    "if not os.path.isdir(os.path.join(experimentFolder,'tailored')):\n",
    "    os.mkdir(os.path.join(experimentFolder,'tailored'))\n",
    "ncFilePath = os.path.join(experimentFolder, 'tailored', ds0.instrument + '.nc')\n",
    "dsTailored.to_netcdf(ncFilePath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save with compression\n",
    "ncFilePath = os.path.join(experimentFolder, 'tailored', ds0.instrument + 'deflate4.nc')\n",
    "encoding = {var: {\"zlib\": True, \"complevel\": 6} for var in dsTailored.data_vars}\n",
    "dsTailored.to_netcdf(ncFilePath, encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape XArray ds\n",
    "\n",
    "# Make full time vector\n",
    "t0 = datetime(2024, 12, 12, 9, 0, 0)\n",
    "t_end = datetime(2024, 12, 27, 16, 20, 0)\n",
    "t_full   = pd.date_range(t0, t_end, freq='{}s'.format(1 / 16)) # 16hz time vector\n",
    "\n",
    "# Add t_full to dsTailered as additional dimension\n",
    "dsTailored = dsTailored.assign_coords(t_full=t_full)\n",
    "dsTailored = dsTailored.expand_dims('t_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a continuous time array\n",
    "t_continuous = np.array([t + np.timedelta64(int(n), 's') for t in ds.t.values for n in ds.N.values])\n",
    "\n",
    "# Flatten the burst structure for plotting\n",
    "zs_flat = ds.zs.values.flatten()\n",
    "zs_low_flat = ds.zs_low.values.flatten()\n",
    "zs_high_flat = ds.zs_high.values.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.plot(t_continuous, zs_flat, label='zs', alpha=0.7, linewidth=0.9)\n",
    "plt.plot(t_continuous, zs_high_flat, label='zs_high', alpha=0.7, linewidth=0.9)\n",
    "plt.plot(t_continuous, zs_low_flat, label='zs_low', alpha=0.7, linewidth=0.9)\n",
    "\n",
    "ds.h.plot(label='Waterlevel')\n",
    "\n",
    "\n",
    "\n",
    "# give values below y=0 a different color\n",
    "# plt.fill_between(t_continuous, zs_flat, 0, where=zs_flat < 0, color='grey', alpha=1, zorder=4, linewidth=1.5)\n",
    "# plt.fill_between(t_continuous, zs_low_flat, 0, where=zs_low_flat < 0, color='grey', alpha=1, zorder=4, linewidth=1.5)\n",
    "# plt.fill_between(t_continuous, zs_high_flat, 0, where=zs_high_flat < 0, color='grey', alpha=1, zorder=4, linewidth=1.5)\n",
    "plt.axhline(y=0, color='k', linestyle='--', label='Bed level')\n",
    "\n",
    "# Set x-axis limits using specific date stamps\n",
    "start_time = pd.Timestamp(\"2024-12-18T12:00:00\")\n",
    "end_time = pd.Timestamp(\"2024-12-21T00:00:00\")\n",
    "# plt.xlim(start_time, end_time)\n",
    "\n",
    "plt.axhline((ds.zi-ds.zb).values, color='k', linestyle='--', label='Instrument height above bed')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Surface Elevation (m)')\n",
    "plt.title('Surface Elevation vs Time RBR06 2024-12-16 to 2024-12-27')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot hmo \n",
    "plt.figure(figsize=(15, 8))\n",
    "ds.Hm0.plot(label='Hm0')\n",
    "ds.Hm0_low.plot(label=f'Hm0_low (mean={ds.Hm0_low.mean().values:.3g} m)')\n",
    "\n",
    "ds.Hm0_high.plot(label='Hm0_high')\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Hm0 (m)')\n",
    "plt.title('Hm0 vs Time')\n",
    "plt.grid(True)\n",
    "\n",
    "# Set x-axis limits using specific date stamps\n",
    "start_time = pd.Timestamp(\"2024-12-18T12:00:00\")\n",
    "end_time = pd.Timestamp(\"2024-12-21T00:00:00\")\n",
    "# plt.xlim(start_time, end_time)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot hs and h\n",
    "plt.figure(figsize=(16, 3))\n",
    "ds.h.plot(label='Waterlevel')\n",
    "# (ds.h + ds.zb).plot(label='Waterlevel + zb (NAP)')\n",
    "\n",
    "# Interpolate between points\n",
    "ds.Hm0.plot(label=f'Hm0 (mean={ds.Hm0.mean().values:.3g} m)')\n",
    "ds.Hm0_low.plot(label=f'Hm0_low (mean={ds.Hm0_low.mean().values:.3g} m)')\n",
    "\n",
    "# ds.Hm0_high.plot(label='Hm0_high')\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('(m)')\n",
    "plt.title('Hm0 vs Time')\n",
    "plt.grid(True)\n",
    "\n",
    "# Set x-axis limits using specific date stamps\n",
    "start_time = pd.Timestamp(\"2024-12-21T16:00:00\")\n",
    "end_time = pd.Timestamp(\"2024-12-23T13:00:00\")\n",
    "# plt.xlim(start_time, end_time)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 111\n",
    "end =120\n",
    "\n",
    "plt.figure(figsize=(16, 2))\n",
    "(ds.zs.isel(t=t)-ds.zs.isel(t=t).mean()).plot(label = f'Water elevation; Hm0: {ds.Hm0.isel(t=t).values:.3g}m')\n",
    "# (ds.zs.isel(t=t)-ds.h.isel(t=t).mean()).plot(label = f'Water elevation; Hm0: {ds.Hm0.isel(t=t).values:.3g}m')\n",
    "plt.grid()\n",
    "plt.xlim(0,end)\n",
    "plt.ylabel(f'\\u03B7 [m]')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.figure(figsize=(16, 2))\n",
    "ds.zs.isel(t=t).plot(color='grey', label= 'total elevation')\n",
    "ds.zs_low.isel(t=t).plot(color='green', label = f'infragravity waves; Hm0: {ds.Hm0_low.isel(t=t).values:.3g}m')\n",
    "plt.grid()\n",
    "plt.xlim(0,end)\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.figure(figsize=(16, 2))\n",
    "ds.zs.isel(t=t).plot(color='grey', label= 'total elevation')\n",
    "ds.zs_high.isel(t=t).plot(color='red', label = f'wind waves; Hm0: {ds.Hm0_high.isel(t=t).values:.3g}m')\n",
    "\n",
    "plt.grid()\n",
    "plt.xlim(0,end)\n",
    "plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to remove outliers\n",
    "# def remove_outliers(data, threshold=3):\n",
    "#     mean = np.mean(data)\n",
    "#     std_dev = np.std(data)\n",
    "#     filtered_data = [x if abs(x - mean) < threshold * std_dev else mean for x in data]\n",
    "#     return np.array(filtered_data)\n",
    "\n",
    "def remove_outliers(data, threshold=3):\n",
    "    mean = np.mean(data)\n",
    "    std_dev = np.std(data)\n",
    "    mask = np.abs(data - mean) < threshold * std_dev\n",
    "    # filtered_data = np.where(mask, data, mean + threshold * std_dev * np.sign(data - mean))  # Replace outliers with closest value within threshold\n",
    "    filtered_data = np.where(mask, data, np.nan) # Replace outliers with nan\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "data = ds.zs.isel(t=t).values  # Extract the data values for the selected time index t\n",
    "\n",
    "# Remove outliers from the data\n",
    "cleaned_data = remove_outliers(data)\n",
    "\n",
    "# Interpolate NaN values in the pressure signal\n",
    "# cleaned_data = cleaned_data.interpolate_na(method='linear')\n",
    "\n",
    "# Calculate the mean of the cleaned data\n",
    "mean_cleaned_data = np.mean(cleaned_data)\n",
    "\n",
    "# Plotting the cleaned data\n",
    "plt.figure(figsize=(16, 2))\n",
    "plt.plot(ds.N, cleaned_data - mean_cleaned_data, label=f'Water elevation; Hm0: {ds.Hm0.isel(t=t).values:.3g}m')\n",
    "plt.grid()\n",
    "plt.xlim(0, 200)\n",
    "plt.ylabel(f'\\u03B7 [m]')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "print(len(cleaned_data)/16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "t=111\n",
    "freq = ds.frequencies.isel(t=t)\n",
    "psd = ds.psd.isel(t=t)\n",
    "\n",
    "\n",
    "# Calculate confidence intervals\n",
    "nBlocks = len(ds.zs.isel(t=t)) // nperseg  # Estimate the number of blocks used by Welch's method\n",
    "edf = round(nBlocks * 2)     # Degrees of freedom (approximately 2 per segment)\n",
    "alpha = 0.1                  # 90% confidence level\n",
    "\n",
    "confLow = edf / chi2.ppf(1 - alpha / 2, edf)  # Lower confidence limit\n",
    "confUpper = edf / chi2.ppf(alpha / 2, edf)    # Upper confidence limit\n",
    "\n",
    "# Confidence interval bounds for PSD\n",
    "psd_lower = psd * confLow\n",
    "psd_upper = psd * confUpper\n",
    "\n",
    "\n",
    "\n",
    "### background plot \n",
    "freq_bg, psd_bg = welch(ds.zs.isel(t=t).values, fs=16, nperseg=19200, detrend='constant', window='hann')\n",
    "nBlocks_bg = len(ds.zs.isel(t=t).values) // 19200  # Estimate the number of blocks used by Welch's method\n",
    "edf_bg = round(nBlocks * 2)     # Degrees of freedom (approximately 2 per segment)\n",
    "confLow_bg = edf / chi2.ppf(1 - alpha / 2, edf)  # Lower confidence limit\n",
    "confUpper_bg = edf / chi2.ppf(alpha / 2, edf)    # Upper confidence limit\n",
    "psd_lower_bg = psd_bg * confLow_bg\n",
    "psd_upper_bg = psd_bg * confUpper_bg\n",
    "# plt.plot(freq_bg, psd_upper_bg, color='gray', linestyle= '--')\n",
    "plt.fill_between(freq_bg, psd_lower_bg, psd_upper_bg, color='gray', alpha=0.3, label='Raw spectrum', linestyle='--')\n",
    "\n",
    "### plotting\n",
    "plt.fill_between(freq, psd_lower, psd_upper, color='black', alpha=0.3, label='90% Confidence Interval')\n",
    "plt.semilogy(freq, psd, label='Power Density Spectrum')\n",
    "\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('PSD (m²/Hz)')\n",
    "plt.title(f'Power Spectral Density using Welch\\'s Method for t ={t}')\n",
    "plt.grid()\n",
    "plt.yscale('linear')\n",
    "# plt.xscale('log')\n",
    "plt.xscale('linear')\n",
    "plt.xlim(0,0.6)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_time_index(ds, specific_time):\n",
    "    \"\"\"\n",
    "    Function to find the index 't' for a specific time in the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    ds (xarray.Dataset): The dataset containing the time dimension.\n",
    "    specific_time (str or datetime): The specific time to find the index for.\n",
    "    \n",
    "    Returns:\n",
    "    int: The index corresponding to the specific time.\n",
    "    \"\"\"\n",
    "    specific_time = np.datetime64(specific_time)\n",
    "    time_index = np.where(ds.t.values == specific_time)[0]\n",
    "    \n",
    "    if len(time_index) == 0:\n",
    "        raise ValueError(f\"Time {specific_time} not found in the dataset.\")\n",
    "    \n",
    "    return time_index[0]\n",
    "\n",
    "# Example usage\n",
    "specific_time = \"2024-12-20T12:00:00\"\n",
    "t_index = find_time_index(ds, specific_time)\n",
    "print(f\"The index for the specific time {specific_time} is {t_index}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lidar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
