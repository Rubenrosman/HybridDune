{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUD-COASTAL instrument processing\n",
    "This notebook will guide you through the data postprocessing of the ADV-instruments. If you have any questions, please contact [m.a.vanderlugt@tudelft.nl].\n",
    "\n",
    "# 1. Read/store raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "sys.path.append(r'c:\\checkouts\\python\\TUD-COASTAL\\instrumentProcessing')\n",
    "import puv\n",
    "from vector import Vector\n",
    "from KNMI_readers import read_knmi_uurgeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data management\n",
    "Before we start processing the data we need to first define the location of the measurement data files, the start and stop time of the measurement window we would like to process and a location to write the netcdf output data to. We will also create some information metadata later on; in this part we will create a name for the instrument we will process. Below you can find an example of the code, please adjust accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of raw data\n",
    "dataFolder = r'c:\\checkouts\\python\\TUD-COASTAL\\instrumentProcessing\\example_data\\ADV\\raw_phzd'\n",
    "# name of the instantiated vector class\n",
    "name = 'vec1'\n",
    "# start time over which to read data (must be larger than first recorded time)\n",
    "tstart = '2020-11-30 17:00:00'\n",
    "# stop time over which to read data (must be smaller than last recorded time)\n",
    "tstop = '2020-12-01 00:00:00'\n",
    "# location of netcdfdata \n",
    "ncOutDir = r'c:\\checkouts\\python\\TUD-COASTAL\\instrumentProcessing\\example_data\\ADV\\raw_netcdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of raw data\n",
    "dataFolder = r'c:\\checkouts\\python\\TUD-COASTAL\\instrumentProcessing\\example_data\\ADV\\raw_phzd'\n",
    "# name of the instantiated vector class\n",
    "name = 'vec1'\n",
    "# start time over which to read data (must be larger than first recorded time)\n",
    "tstart = '2020-11-30 17:00:00'\n",
    "# stop time over which to read data (must be smaller than last recorded time)\n",
    "# tstop = '2020-12-02 17:00:00'\n",
    "tstop = '2020-12-01 00:00:00'\n",
    "# location of netcdfdata \n",
    "ncOutDir = r'c:\\checkouts\\python\\TUD-COASTAL\\instrumentProcessing\\example_data\\ADV\\raw_netcdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data\n",
    "In this part we create a vec object, based on the Vector class. The class holds all sorts of functions and variables which we will use to process the raw data and transform it into the netcdf data used for further processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".dat file was read\n",
      ".sen file was read\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>w</th>\n",
       "      <th>p</th>\n",
       "      <th>anl1</th>\n",
       "      <th>anl2</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>snr1</th>\n",
       "      <th>snr2</th>\n",
       "      <th>snr3</th>\n",
       "      <th>cor1</th>\n",
       "      <th>cor2</th>\n",
       "      <th>cor3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-11-30 17:00:00.000000</th>\n",
       "      <td>-0.167</td>\n",
       "      <td>0.211</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>11520.0</td>\n",
       "      <td>1615.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>45.1</td>\n",
       "      <td>45.6</td>\n",
       "      <td>44.7</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-30 17:00:00.062500</th>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>11530.0</td>\n",
       "      <td>1605.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>46.4</td>\n",
       "      <td>44.3</td>\n",
       "      <td>47.3</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-30 17:00:00.125000</th>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.222</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>11560.0</td>\n",
       "      <td>1597.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>43.9</td>\n",
       "      <td>44.3</td>\n",
       "      <td>46.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-30 17:00:00.187500</th>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.002</td>\n",
       "      <td>11550.0</td>\n",
       "      <td>1619.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>45.1</td>\n",
       "      <td>45.6</td>\n",
       "      <td>45.6</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-30 17:00:00.250000</th>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.003</td>\n",
       "      <td>11550.0</td>\n",
       "      <td>1607.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>44.7</td>\n",
       "      <td>44.7</td>\n",
       "      <td>47.7</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-30 23:59:59.687500</th>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>0.015</td>\n",
       "      <td>9740.0</td>\n",
       "      <td>6800.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>46.4</td>\n",
       "      <td>48.2</td>\n",
       "      <td>49.4</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-30 23:59:59.750000</th>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>0.013</td>\n",
       "      <td>9730.0</td>\n",
       "      <td>6768.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>48.2</td>\n",
       "      <td>45.6</td>\n",
       "      <td>50.3</td>\n",
       "      <td>99.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-30 23:59:59.812500</th>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>9710.0</td>\n",
       "      <td>6775.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>48.6</td>\n",
       "      <td>49.4</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-30 23:59:59.875000</th>\n",
       "      <td>0.163</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>9740.0</td>\n",
       "      <td>6821.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>46.4</td>\n",
       "      <td>48.2</td>\n",
       "      <td>48.6</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-30 23:59:59.937500</th>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>9760.0</td>\n",
       "      <td>6787.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>47.3</td>\n",
       "      <td>47.7</td>\n",
       "      <td>46.9</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>403200 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                u      v      w        p    anl1  anl2     a1  \\\n",
       "t                                                                               \n",
       "2020-11-30 17:00:00.000000 -0.167  0.211 -0.020  11520.0  1615.0   0.0  156.0   \n",
       "2020-11-30 17:00:00.062500 -0.165  0.208 -0.026  11530.0  1605.0   0.0  159.0   \n",
       "2020-11-30 17:00:00.125000 -0.149  0.222 -0.003  11560.0  1597.0   0.0  153.0   \n",
       "2020-11-30 17:00:00.187500 -0.148  0.231  0.002  11550.0  1619.0   0.0  156.0   \n",
       "2020-11-30 17:00:00.250000 -0.137  0.233  0.003  11550.0  1607.0   0.0  155.0   \n",
       "...                           ...    ...    ...      ...     ...   ...    ...   \n",
       "2020-11-30 23:59:59.687500  0.132 -0.242  0.015   9740.0  6800.0   0.0  159.0   \n",
       "2020-11-30 23:59:59.750000  0.118 -0.235  0.013   9730.0  6768.0   0.0  163.0   \n",
       "2020-11-30 23:59:59.812500  0.134 -0.222 -0.005   9710.0  6775.0   0.0  158.0   \n",
       "2020-11-30 23:59:59.875000  0.163 -0.208 -0.016   9740.0  6821.0   0.0  159.0   \n",
       "2020-11-30 23:59:59.937500  0.132 -0.187 -0.018   9760.0  6787.0   0.0  161.0   \n",
       "\n",
       "                               a2     a3  snr1  snr2  snr3  cor1  cor2  cor3  \n",
       "t                                                                             \n",
       "2020-11-30 17:00:00.000000  157.0  154.0  45.1  45.6  44.7  99.0  99.0  99.0  \n",
       "2020-11-30 17:00:00.062500  154.0  160.0  46.4  44.3  47.3  99.0  99.0  98.0  \n",
       "2020-11-30 17:00:00.125000  154.0  157.0  43.9  44.3  46.0  99.0  99.0  99.0  \n",
       "2020-11-30 17:00:00.187500  157.0  156.0  45.1  45.6  45.6  99.0  99.0  99.0  \n",
       "2020-11-30 17:00:00.250000  155.0  161.0  44.7  44.7  47.7  99.0  99.0  98.0  \n",
       "...                           ...    ...   ...   ...   ...   ...   ...   ...  \n",
       "2020-11-30 23:59:59.687500  163.0  165.0  46.4  48.2  49.4  99.0  99.0  99.0  \n",
       "2020-11-30 23:59:59.750000  157.0  167.0  48.2  45.6  50.3  99.0  98.0  99.0  \n",
       "2020-11-30 23:59:59.812500  164.0  165.0  46.0  48.6  49.4  99.0  99.0  99.0  \n",
       "2020-11-30 23:59:59.875000  163.0  163.0  46.4  48.2  48.6  99.0  99.0  99.0  \n",
       "2020-11-30 23:59:59.937500  162.0  159.0  47.3  47.7  46.9  99.0  99.0  98.0  \n",
       "\n",
       "[403200 rows x 15 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw data to netcdf; create a vector object 'vec'\n",
    "vec = Vector(name, dataFolder, tstart=tstart, tstop=tstop)\n",
    "\n",
    "# reads the raw data from tstart to tstop and casts all data in a pandas DataFrame that is stored under vec.dfpuv.\n",
    "# in case there is no data between tstart and tstop the DataFrame is not instantiated\n",
    "vec.read_raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break up the data into burst blocks\n",
    "vec.cast_to_blocks_in_xarray(blockWidth=600)\n",
    "\n",
    "# compute burst averages (make sure to read vector.py what is happening exactly!)\n",
    "vec.compute_block_averages()\n",
    "\n",
    "# all data is collected in an xarray Dataset ds. We extract this from the class instantiation and\n",
    "# we can easily write it to netCDF\n",
    "ds = vec.ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create metadata\n",
    "This part will create the metadata. Adding information to the measurement will help in the later part of postprocessing to keep track of which measurement you are working on. Filling it in is not necessary perse, but it will lessen the paper administration later on. Choose your adminstrative battles wisely. The example has been omitted, please replace the code below with your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add global attribute metadata\n",
    "ds.attrs = {'Conventions': 'CF-1.6',\n",
    "            'title': '{}'.format(vec.name),\n",
    "            'instrument': '{}'.format('vec1'),\n",
    "            'instrument serial number': '{}'.format(16725),\n",
    "            'epsg': 28992,\n",
    "            'x': 117196.6,\n",
    "            'y': 559818.2,\n",
    "            'time zone': 'UTC+2',\n",
    "            'coordinate type': 'XYZ',\n",
    "            'summary': 'December pilot field campaign',\n",
    "            'contact person': 'Marlies van der Lugt',\n",
    "            'emailadres': 'm.a.vanderlugt@tudelft.nl',\n",
    "            'construction datetime': datetime.now().strftime(\"%d-%b-%Y (%H:%M:%S)\"),\n",
    "            'version': 'v1',\n",
    "            'version comments': 'constructed with xarray'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the data\n",
    "To reduce the size of the data we will compress the data and use listcomprehension to apply it to the dataset. After this step we save the dataset in a netcdf format in the output folder we previously created. The code will check whether this folder exists or not and create one if it is not in your path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify compression for all the variables to reduce file size\n",
    "comp = dict(zlib=True, complevel=5)\n",
    "ds.encoding = {var: comp for var in ds.data_vars}\n",
    "\n",
    "# save to netCDF\n",
    "if not os.path.exists(ncOutDir):\n",
    "    os.mkdir(ncOutDir)\n",
    "ds.to_netcdf(ncOutDir + r'\\{}_pilot_short.nc'.format(vec.name))\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Quality control adv\n",
    "The raw data has been transformed into the netcdf format. Now, the quality of the data is checked. We define some quality control parameters, and detect outliers based on these parameters. The data will also be corrected for weather conditions and its position. First we start with defining the general settings and parameters for quality control. Please check all the parameters to fit your measurement data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if skipping the code above, one can immediately load the raw data from netcdf\n",
    "ds = xr.open_dataset(r'c:\\checkouts\\python\\TUD-COASTAL\\instrumentProcessing\\example_data\\ADV\\raw_netcdf\\vec1_pilot_short.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# height of instrument above bed\n",
    "hi = 0.57 #m\n",
    "# height of instruments pressure sensor above bed\n",
    "hip = 0.27 #m\n",
    "# bed level\n",
    "zb = -1.13 #m NAP\n",
    "# angle of x-pod of the vector head with respect to north (clockwise positive)\n",
    "thet = 45\n",
    "# density of water\n",
    "rho = 1025 # kg/m3\n",
    "# gravitational acceleration\n",
    "g = 9.81 # m/s2\n",
    "# parameters for the quality control:\n",
    "QC = {\n",
    "     'uLim':2.1, #maximum acceptable recorded u-velocity\n",
    "     'vLim':2.1, #maximum acceptable recorded v-velocity\n",
    "     'wLim':0.6, #maximum acceptable recorded w-velocity\n",
    "     'corTreshold':70, #minimum correlation\n",
    "     'maxFracNans': 0.02, #maximum fraction of rejected pings in the sample to proceed with processing based on interpolation\n",
    "     'maxGap' : 4 #maximum amount of sequential rejected pings in the sample to proceed with processing based on interpolation\n",
    "      }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add data parameters\n",
    "At this point we will add some datacolumns to the existing dataset, based on the general settings defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % add some data to the dataset\n",
    "ds['zb'] = zb\n",
    "ds['zb'].attrs = {'units': 'm+NAP', 'long_name': 'bed level'}\n",
    "\n",
    "ds['zi'] = ds['zb'] + hi\n",
    "ds['zi'].attrs = {'units': 'm+NAP', 'long_name': 'position probe'}\n",
    "\n",
    "ds['zip'] = ds['zb'] + hip\n",
    "ds['zip'].attrs = {'units': 'm+NAP', 'long_name': 'position pressure sensor'}\n",
    "\n",
    "ds['rho'] = rho\n",
    "ds['rho'].attrs = {'units': 'kg/m3', 'long_name': 'water density'}\n",
    "\n",
    "ds['g'] = g\n",
    "ds['g'].attrs = {'units': 'm', 'long_name': 'gravitational acceleration'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality control threshold and outlier detection\n",
    "The pressure and velocity is now checked if it fits within the previously defined confidence range and if the observations are within range. Outliers are detected and removed from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if correlation is outside confidence range\n",
    "mc1 = ds.cor1 > QC['corTreshold']\n",
    "mc2 = ds.cor2 > QC['corTreshold']\n",
    "mc3 = ds.cor3 > QC['corTreshold']\n",
    "\n",
    "# if observation is outside of velocity range\n",
    "mu1 = np.abs(ds.u) < QC['uLim']\n",
    "mu2 = np.abs(ds.v) < QC['uLim']\n",
    "mu3 = np.abs(ds.w) < QC['uLim']\n",
    "\n",
    "# if du larger than 4*std(u) then we consider it outlier and hence remove:\n",
    "md1 = np.abs(ds.u.diff('N')) < 3 * ds.u.std(dim='N')\n",
    "md1 = md1.combine_first(mu1)\n",
    "md2 = np.abs(ds.v.diff('N')) < 3 * ds.v.std(dim='N')\n",
    "md2 = md1.combine_first(mu2)\n",
    "md3 = np.abs(ds.w.diff('N')) < 3 * ds.w.std(dim='N')\n",
    "md3 = md1.combine_first(mu3)\n",
    "\n",
    "ds['mc'] = np.logical_and(np.logical_and(mc1, mc2), mc3)\n",
    "ds['mu'] = np.logical_and(np.logical_and(mu1, mu2), mu3)\n",
    "ds['md'] = np.logical_and(np.logical_and(md1, md2), md3)\n",
    "ds['mc'].attrs = {'units': '-', 'long_name': 'mask correlation'}\n",
    "ds['mu'].attrs = {'units': '-', 'long_name': 'mask vel limit'}\n",
    "ds['md'].attrs = {'units': '-', 'long_name': 'mask deviation'}\n",
    "\n",
    "# if dp larger than 4*std(p) then we consider it outlier and hence remove:\n",
    "mp = np.abs(ds.p.diff('N')) < 4 * ds.p.std(dim='N')\n",
    "mp = xr.concat([mp.isel(N=0), mp], dim=\"N\")\n",
    "\n",
    "# add the mask variables as coordinates to the dataset\n",
    "ds.coords['maskp'] = (('t', 'N'), mp.values)\n",
    "ds.coords['maskv'] = (('t', 'N'), np.logical_and(np.logical_and(ds.mc.values, ds.mu.values), ds.md.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct the data\n",
    "The next step is correcting for air pressure fluctuations and drift. This will be done using the knmi file we loaded before.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of knmi file (to correct for air pressure drift during the experiment)\n",
    "knmiFile = r'c:\\checkouts\\python\\TUD-COASTAL\\instrumentProcessing\\example_data\\KNMI_20201208_hourly.txt'\n",
    "# number of the knmi station (to make sure the correction is done with the correct KNMI station)\n",
    "stationNumber = 235"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct for the air pressure fluctuations and drift in the instrument\n",
    "# first we load the data and add it to the dataset\n",
    "dfp = read_knmi_uurgeg(\n",
    "    knmiFile,\n",
    "    stationNumber)\n",
    "dt = ((ds.t[1] - ds.t[0]) / np.timedelta64(1, 's')).values\n",
    "pAir = dfp['P'].to_xarray().resample({'t': '{}S'.format(dt)}).interpolate('linear')\n",
    "ds['pAir'] = pAir.sel(t=slice(ds.t.min(), ds.t.max()))\n",
    "\n",
    "# we correct for drift in air pressure, nothing else\n",
    "ds['dpAir'] = ds['pAir'] - ds['pAir'].isel(t=0)\n",
    "\n",
    "# correct the pressure signal with dpAir and with drift in instrument pressure\n",
    "ds['pc'] = ds['p'] - ds['dpAir']\n",
    "ds['pc'].attrs = {'units': 'Pa + NAP', 'long_name': 'pressure', 'comments': 'drift in air pressure is corrected'}\n",
    "\n",
    "ds['eta'] = ds['pc'] / rho / g + ds.zip\n",
    "ds['eta'].attrs = {'units': 'm+NAP', 'long_name': 'hydrostatic water level'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean water level and water depth\n",
    "ds['zsmean'] = ds.eta.mean(dim='N')\n",
    "ds['zsmean'].attrs = {'units': 'm + NAP', 'long_name': 'water level',\n",
    "                      'comments': 'burst averaged'}\n",
    "\n",
    "ds['h'] = ds.zsmean - zb\n",
    "ds['h'].attrs = {'units': 'm', 'long_name': 'water column height'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is corrected we can rotate the data to ENU coordinate system. If the data is already measured in ENU, nothing needs to be done. After rotating we also remove data that is below the location of the sensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #% rotate to ENU coordinates (this is only necessary if measurements were performed in XYZ coordinate system)\n",
    "ufunc = lambda u,v: puv.rotate_velocities(u,v,thet-90)\n",
    "ds['u'],ds['v'] = xr.apply_ufunc(ufunc,\n",
    "                    ds['u'], ds['v'],\n",
    "                    input_core_dims=[['N'], ['N']],\n",
    "                    output_core_dims=[['N'],['N']],\n",
    "                    vectorize=True)\n",
    "ds['u'].attrs = {'units':'m/s','long_name':'velocity E'}\n",
    "ds['v'].attrs = {'units':'m/s','long_name':'velocity N'}\n",
    "ds['w'].attrs = {'units':'m/s','long_name':'velocity U'}\n",
    "\n",
    "# remove pressure observations where the estimated water level is\n",
    "# lower than the sensor height with margin of error of 10 cm\n",
    "ds.coords['maskd'] = (('t', 'N'), zb+hi < (ds['eta'].values - 0.1))\n",
    "ds[['u','v','w','p','pc','eta']] = ds[['u','v','w','p','pc','eta']].where(ds.maskp == True)\n",
    "ds[['u','v','w','p','pc','eta']] = ds[['u','v','w','p','pc','eta']].where(ds.maskd == True)\n",
    "ds[['u','v','w','p','pc','eta']] = ds[['u','v','w','p','pc','eta']].where(ds.maskv == True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata update\n",
    "The metadata will be updated after correcting the data. Version number goes up and extra information on the corrections made will be added. We can also omit the sen data, so this will be deleted from the dataset. Finally, the data will be compressed and saved to the directory for output we defined previously. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ammending the meta data to add extra info\n",
    "ds.attrs['version'] = 'v2'\n",
    "ds.attrs['coordinate type'] = 'ENU'\n",
    "ds.attrs['comment'] = 'Quality checked data: pressure reference level corrected for airpressure drift,' + \\\n",
    "                 r'correlation and amplitude checks done and spikes were removed. ' + \\\n",
    "                 r'Velocities rotated to ENU coordinates based on heading and configuration in the field.'\n",
    "\n",
    "# save to netCDF wwhere we don't include the sen data any more because we have only used it for the quality check\n",
    "ds = ds.drop(['a1', 'a2', 'a3',\n",
    "              'cor1', 'cor2', 'cor3',\n",
    "              'snr1', 'snr2', 'snr3',\n",
    "              'heading', 'pitch', 'roll',\n",
    "              'voltage', 'pc'])\n",
    "\n",
    "# specify compression for all the variables to reduce file size\n",
    "comp = dict(zlib=True, complevel=5)\n",
    "ds.encoding = {var: comp for var in ds.data_vars}\n",
    "ncOutDir = r'c:\\checkouts\\python\\TUD-COASTAL\\instrumentProcessing\\example_data\\ADV\\qc'\n",
    "if not os.path.exists(ncOutDir):\n",
    "    os.mkdir(ncOutDir)\n",
    "ds.to_netcdf(os.path.join(ncOutDir, 'vec1_short.nc'), encoding=ds.encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Compute wave statistics\n",
    "The final notebook will compute the wave statistics of the measurements. The output will be a slimmed down dataset with wave characteristics based on the pressures measured. All other data will be removed as it is already present in the raw data. This will speed up the final dataprocessing. We start with defining the inputparameters.\n",
    "\n",
    "### Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input specification\n",
    "instrFile = r'c:\\checkouts\\python\\TUD-COASTAL\\instrumentProcessing\\example_data\\ADV\\qc\\vec1_short.nc'\n",
    " \n",
    "# frequency resolution in fourier space\n",
    "fresolution = 0.03125\n",
    "#number of directional bins \n",
    "ntheta = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "We load the data we prepared in the previous step. The defined bursts will be interpolated and filtered on nan-values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the raw data from netcdf\n",
    "ds0 = xr.open_dataset(instrFile).load()\n",
    "\n",
    "# interpolate nans\n",
    "N = len(ds0.N)\n",
    "for var in ['u', 'v', 'p', 'eta']:\n",
    "    # interpolate the bursts where there is less than 5% nans\n",
    "    data = ds0[var].where(\n",
    "        np.isnan(ds0[var]).sum(dim='N') < 0.05 * len(ds0.N)\n",
    "    ).dropna(dim='t', how='all')\n",
    "    if len(data.t) != 0:\n",
    "        ds0[var] = data.interpolate_na(\n",
    "            dim='N',\n",
    "            method='cubic',\n",
    "            max_gap=8)\n",
    "\n",
    "    # and fill the gaps more than 8 in length with the burst average\n",
    "    ds0[var] = ds0[var].fillna(ds0[var].mean(dim='N'))\n",
    "\n",
    "ds0 = ds0.dropna(dim='t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New dataset\n",
    "A new dataset will be created to provide for wave analysis. New columns are added to calculate in the frequency domain. Other information will be copied from the ds0 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new dataset that has an extra dimension to accomodate for the frequency axis\n",
    "ds = xr.Dataset(data_vars={},\n",
    "          coords = {'t': ds0.t.values,\n",
    "                    'N': ds0.N.values,\n",
    "                    'f': np.arange(0, ds0.sf.values/2, fresolution),\n",
    "                    'theta': np.arange(start=-np.pi,stop=np.pi,step=2*np.pi/ntheta)})\n",
    "ds['f'].attrs = {'units': 'Hz'}\n",
    "ds.attrs = ds0.attrs\n",
    "\n",
    "# put all variables in this new dataset\n",
    "for key in ds0.data_vars:\n",
    "    ds[key] = ds0[key]\n",
    "\n",
    "\n",
    "# extract sampling frequency as explicit variable\n",
    "sf = ds.f.values          \n",
    "\n",
    "# compute water depth\n",
    "ds['h'] = ds['zsmean']-ds['zb']   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wave characteristics\n",
    "Next step is calculating the wave characteristics. The spectrum will be calculated based on pressure measurements; from this spectrum, the spectral density and peak frequency will be derived. Using these, we calculate the main wave characteristics like significant wave height, peak period and the mean wave periods Tm01, Tm02 ad Tmm10. A smoothed peak period will also be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute wave spectra from pressure signal\n",
    "# by applying the linear transfer function to translate the pressure signal to the water surface\n",
    "ufunc = lambda x, h: puv.attenuation_corrected_wave_spectrum(\n",
    "    'pressure',\n",
    "    ds.sf.values, x, h,\n",
    "    ds.zi.values,\n",
    "    ds.zb.values,\n",
    "    fresolution=fresolution)\n",
    "\n",
    "# spectral density\n",
    "fx, ds['vy'] = xr.apply_ufunc(ufunc,\n",
    "                        ds['p'], ds['h'],\n",
    "                        input_core_dims=[['N'], []],\n",
    "                        output_core_dims=[['f'], ['f']],\n",
    "                        vectorize=True) \n",
    "ds['vy'].attrs = {'units': 'm2/Hz', 'long_name': 'spectral density'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute spectral statistics               \n",
    "\n",
    "# peak frequency\n",
    "ufunc = lambda vy: puv.get_peak_frequency(ds.f.values, vy)\n",
    "ds['fp'] = xr.apply_ufunc(ufunc,\n",
    "                        ds['vy'],\n",
    "                        input_core_dims=[['f']],\n",
    "                        output_core_dims=[[]], \n",
    "                        vectorize=True) \n",
    "\n",
    "# wave characteristics \n",
    "ufunc = lambda vy, fp: puv.compute_wave_params(ds.f.values, vy, fmin=0.5*fp, fmax=5)\n",
    "ds['Hm0'], ds['Tp'], ds['Tm01'], ds['Tm02'], ds['Tmm10'], ds['Tps'] = xr.apply_ufunc(ufunc,\n",
    "                        ds['vy'], ds['fp'],\n",
    "                        input_core_dims=[['f'], []],\n",
    "                        output_core_dims=[[], [], [], [], [], []],\n",
    "                        vectorize=True) \n",
    "ds['Hm0'].attrs = {'units': 'm', 'long_name': 'significant wave height','computation':'computed between fmin=0.5fp and fmax=5'}\n",
    "ds['Tp'].attrs = {'units': 's', 'long_name': 'peak wave period','computation':'computed between fmin=0.5fp and fmax=5'}\n",
    "ds['Tm01'].attrs = {'units': 's', 'long_name': 'mean wave period','computation':'computed between fmin=0.5fp and fmax=5'}\n",
    "ds['Tm02'].attrs = {'units': 's', 'long_name': 'mean wave period','computation':'computed between fmin=0.5fp and fmax=5'}\n",
    "ds['Tmm10'].attrs = {'units': 's', 'long_name': 'mean wave period','computation':'computed between fmin=0.5fp and fmax=5'}\n",
    "ds['Tps'].attrs = {'units': 's', 'long_name': 'peak wave period','computation':'computed between fmin=0.5fp and fmax=5', 'comment':'smoothed estimate from the discrete spectrum'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wave characteristics time domain\n",
    "We also calculate the directional wave spectra based on the velocities in the time domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute current magnitudes and direction all computed in the time domain\n",
    "ds['u_mean'] = ds.u.mean(axis=1)\n",
    "ds['u_mean'].attrs = {'units': 'm/s', 'long_name': 'current x-component', 'computation': 'burst averaged'}\n",
    "\n",
    "ds['v_mean'] = ds.v.mean(axis=1)\n",
    "ds['v_mean'].attrs = {'units': 'm/s', 'long_name': 'current y-component', 'computation': 'burst averaged'}\n",
    "               \n",
    "ds['cur_dir'] = np.arctan2(ds['v_mean'], ds['u_mean'])*180/np.pi\n",
    "ds['cur_dir'].attrs = {'units': 'deg', 'long_name': 'current direction, cartesian convention'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Wave characteristics from method of maximum entropy\n",
    "We also calculate the directional wave spectra based on the phase coupling between pressure and velocities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directional wave spectra\n",
    "ufunc = lambda p, u, v, h, fp: puv.wave_MEMpuv(p/1e4, u, v, h,\n",
    "                    ds.zi.values,\n",
    "                    ds.zb.values,\n",
    "                    ds.sf.values,\n",
    "                    fresolution=fresolution,\n",
    "                    ntheta=ntheta,\n",
    "                    fcorrmin=0.5*fp,\n",
    "                    fcorrmax=5,\n",
    "                    maxiter=20)\n",
    "            \n",
    "fx, vy, theta, ds['S'] = xr.apply_ufunc(ufunc,\n",
    "                        ds['p'], ds['u'], ds['v'], ds['h'], ds['fp'],\n",
    "                        input_core_dims=[['N'], ['N'], ['N'], [], []],\n",
    "                        output_core_dims=[['f'], ['f'], ['theta'], ['f', 'theta']],\n",
    "                        vectorize=True) \n",
    "ds['S'].attrs = {'units': 'm2/Hz/rad', 'long_name': 'directional variance density',\n",
    "                 'computation': 'computed between fmin=0.5fp and fmax=5'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marliesvanderl\\Miniconda3\\envs\\work2\\Lib\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\marliesvanderl\\Miniconda3\\envs\\work2\\Lib\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\marliesvanderl\\Miniconda3\\envs\\work2\\Lib\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# statistics from directional wave spectra\n",
    "ufunc = lambda vy,S,fp: puv.compute_wave_params(ds.f.values, vy, fmin=0.5*fp, fmax=5, theta=ds.theta.values, S=S)\n",
    "Hm0, Tp, Tm01, Tm02, Tmm10, Tps, ds['wavedirmean'],ds['dirspread'] = xr.apply_ufunc(ufunc,\n",
    "                        ds['vy'], ds['S'], ds['fp'],\n",
    "                        input_core_dims=[['f'], ['f', 'theta'], []],\n",
    "                        output_core_dims=[[], [], [], [], [], [], [], []],\n",
    "                        vectorize=True) \n",
    "ds['wavedirmean'].attrs = {'units': 'deg', 'long_name': 'mean wave direction', 'computation': 'computed between fmin=0.5fp and fmax=5'}\n",
    "ds['dirspread'].attrs = {'units': 'deg', 'long_name': 'directional spreading', 'computation': 'computed between fmin=0.5fp and fmax=5'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "The new data will be saved in the output file. The old data will be deleted, as stated previously, to save memory and to reduce duplicate data. You will end up with three different files. The raw data, the corrected/filtered data and the wave characteristics data, all in netcdf format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "ncOutFile = r'c:\\checkouts\\python\\TUD-COASTAL\\instrumentProcessing\\example_data\\ADV\\tailored\\vec1_pilot_tailored_short.nc'\n",
    "\n",
    "# we strip all information on burst scale from the dataset to reduce size (and this info is already present in the raw_netcdf version of the data)\n",
    "dsTailored = ds.drop_dims('N')\n",
    "dsTailored.to_netcdf(ncOutFile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
